<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>jenningL</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="这是RNN教程的第三部分。
在之前的教程中，我们从零开始实现了一个RNN，但没有深入Backpropagation Through Time(BPTT)是怎么计算梯度的细节。在这个部分中，我们将给出BPTT的概述以及它和传统反向传播的不同。然后我们会尝试理解vanishing gradient problem，LSTM和GRU，两个目前在NLP和其他领域最为流行的模型，就是为了解决它而产生的。va">
<meta property="og:type" content="article">
<meta property="og:title" content="jenningL">
<meta property="og:url" content="http://yoursite.com/2016/10/27/RNN教程，part3，BPTT and Vanishing Gradients/index.html">
<meta property="og:site_name" content="jenningL">
<meta property="og:description" content="这是RNN教程的第三部分。
在之前的教程中，我们从零开始实现了一个RNN，但没有深入Backpropagation Through Time(BPTT)是怎么计算梯度的细节。在这个部分中，我们将给出BPTT的概述以及它和传统反向传播的不同。然后我们会尝试理解vanishing gradient problem，LSTM和GRU，两个目前在NLP和其他领域最为流行的模型，就是为了解决它而产生的。va">
<meta property="og:image" content="http://yoursite.com/../img/rnn14.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn15.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn16.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn17.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn18.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn19.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn20.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn21.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn22.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn23.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn21.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn24.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn25.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn27.png">
<meta property="og:image" content="http://yoursite.com/../img/rnn28.png">
<meta property="og:updated_time" content="2016-10-27T12:11:32.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="jenningL">
<meta name="twitter:description" content="这是RNN教程的第三部分。
在之前的教程中，我们从零开始实现了一个RNN，但没有深入Backpropagation Through Time(BPTT)是怎么计算梯度的细节。在这个部分中，我们将给出BPTT的概述以及它和传统反向传播的不同。然后我们会尝试理解vanishing gradient problem，LSTM和GRU，两个目前在NLP和其他领域最为流行的模型，就是为了解决它而产生的。va">
  
    <link rel="alternative" href="/atom.xml" title="jenningL" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">jenningL</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-RNN教程，part3，BPTT and Vanishing Gradients" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/27/RNN教程，part3，BPTT and Vanishing Gradients/" class="article-date">
  <time datetime="2016-10-27T12:11:32.000Z" itemprop="datePublished">2016-10-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这是RNN教程的第三部分。</p>
<p>在之前的教程中，我们从零开始实现了一个RNN，但没有深入Backpropagation Through Time(BPTT)是怎么计算梯度的细节。在这个部分中，我们将给出BPTT的概述以及它和传统反向传播的不同。然后我们会尝试理解vanishing gradient problem，LSTM和GRU，两个目前在NLP和其他领域最为流行的模型，就是为了解决它而产生的。vanishing gradient problem 由Sepp Hochreiter 在1991年首次发现，并且随着深度学习架构的广泛使用不断得到关注。</p>
<p>为了完全理解这部分教程，我建议读者熟悉偏微分和基本的反向传播原理。如果不是的话，你可以在<a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="external">这</a>，<a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="external">这</a>，<a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">这</a>看到几篇很好的教程，难度递增。</p>
<h2 id="BACKPROPAGATION_THROUGH_TIME_(BPTT)">BACKPROPAGATION THROUGH TIME (BPTT)</h2><p>让我们快速回忆一下RNN的基本公式。注意到，为了和其他参考文献保持一致，我把o改成了y^。</p>
<p><img src="../img/rnn14.png" alt=""></p>
<p>我们同样定义了损失（错误）为交叉熵损失：</p>
<p><img src="../img/rnn15.png" alt=""></p>
<p>在这里，yt是在时间步t的正确输出，y^t是我们的预测。我们通常把整个句子看作一个训练样本，因此总误差是每一步的误差的总和。</p>
<p><img src="../img/rnn16.png" alt=""></p>
<p>记住，我们的目标是计算参数U，V，W造成的误差的梯度，然后使用Stochastic Gradient Descent学习到一个良好的模型。跟我们把误差累加一样，我们同样累加在一个训练样本里所有时间步的梯度：<img src="../img/rnn17.png" alt=""><br>为了计算这些梯度，我们使用微分的链式法则。在剩余的篇章里，为了更具体，我将使用E3作为例子。</p>
<p><img src="../img/rnn18.png" alt=""></p>
<p>在上面的式子中，z3 = Vs3，圈叉是两个向量的外积。这里的重点是，求这个微分只和当前时间步有关。但是对于这个式子 <img src="../img/rnn19.png" alt="">则不一样了，注意到 <img src="../img/rnn20.png" alt="">依赖于s2，s2又依赖于s1，以此类推。所以当求W的微分时，我们不能把s2当做常数。我们需要使用再次使用链式法则：</p>
<p><img src="../img/rnn21.png" alt=""> </p>
<p>我们将每个时间步的贡献累加到梯度。换句话说，因为W在每个时间步中都会用到，我们需要在网络中从t=3反向传播梯度到t=0:</p>
<p><img src="../img/rnn22.png" alt=""> </p>
<p>注意到这和deep Feedforward Neural Networks中使用的标准反向传播算法完全一样。关键的不同点在于我们把不同时间步的W的梯度累加起来。在传统的NN，我们在不同层之间不共享参数，所以我们不需要累加任何东西。但是在我的观点里，BPTT只是 在未展开的RNN中的标准反向传播的别名。就和反向传播一样，你可以定义一个delta向量用来反向传递，e.g:<img src="../img/rnn23.png" alt="">。公式同样适用。</p>
<p>一个简陋的BPTT实现如下：</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bptt</span></span>(<span class="keyword">self</span>, x, y)<span class="symbol">:</span></span><br><span class="line">    <span class="constant">T </span>= len(y)</span><br><span class="line">    <span class="comment"># Perform forward propagation</span></span><br><span class="line">    o, s = <span class="keyword">self</span>.forward_propagation(x)</span><br><span class="line">    <span class="comment"># We accumulate the gradients in these variables</span></span><br><span class="line">    dLdU = np.zeros(<span class="keyword">self</span>.<span class="constant">U.</span>shape)</span><br><span class="line">    dLdV = np.zeros(<span class="keyword">self</span>.<span class="constant">V.</span>shape)</span><br><span class="line">    dLdW = np.zeros(<span class="keyword">self</span>.<span class="constant">W.</span>shape)</span><br><span class="line">    delta_o = o</span><br><span class="line">    delta_o[np.arange(len(y)), y] -= <span class="number">1</span>.</span><br><span class="line">    <span class="comment"># For each output backwards...</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(<span class="constant">T)</span>[<span class="symbol">:</span><span class="symbol">:-</span><span class="number">1</span>]<span class="symbol">:</span></span><br><span class="line">        dLdV += np.outer(delta_o[t], s[t].<span class="constant">T)</span></span><br><span class="line">        <span class="comment"># Initial delta calculation: dL/dz</span></span><br><span class="line">        delta_t = <span class="keyword">self</span>.<span class="constant">V.T.</span>dot(delta_o[t]) * (<span class="number">1</span> - (s[t] ** <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># Backpropagation through time (for at most self.bptt_truncate steps)</span></span><br><span class="line">        <span class="keyword">for</span> bptt_step <span class="keyword">in</span> np.arange(max(<span class="number">0</span>, t-<span class="keyword">self</span>.bptt_truncate), t+<span class="number">1</span>)[<span class="symbol">:</span><span class="symbol">:-</span><span class="number">1</span>]<span class="symbol">:</span></span><br><span class="line">            <span class="comment"># print "Backpropagation step t=%d bptt step=%d " % (t, bptt_step)</span></span><br><span class="line">            <span class="comment"># Add to gradients at each previous step</span></span><br><span class="line">            dLdW += np.outer(delta_t, s[bptt_step-<span class="number">1</span>])              </span><br><span class="line">            dLdU[<span class="symbol">:</span>,x[bptt_step]] += delta_t</span><br><span class="line">            <span class="comment"># Update delta for next step dL/dz at t-1</span></span><br><span class="line">            delta_t = <span class="keyword">self</span>.<span class="constant">W.T.</span>dot(delta_t) * (<span class="number">1</span> - s[bptt_step-<span class="number">1</span>] ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> [dLdU, dLdV, dLdW]</span><br></pre></td></tr></table></figure>
<p>这应该给了你关于传统RNN为什么很难训练的感受：序列可以很长，所以你需要回溯很多层。在实际应用中，很多人会限制只回溯几层。</p>
<h2 id="VANISHING_GRADIENT_PROBLEM">VANISHING GRADIENT PROBLEM</h2><p>在之前的教程中，我们提到过RNN在学习长距离依赖上有困难。这是有问题的，因为英文句子的意思往往由相距较远的词语决定的。“The man who wore a wig on his head went inside” 这个句式实际上是关于一个男人（man）走进来，而不是一顶假发（wig）。但RNN不太可能捕捉到这样的信息。为了理解其中的原因，让我们来仔细看一下我们前文计算的梯度：</p>
<p><img src="../img/rnn21.png" alt=""></p>
<p>注意到<img src="../img/rnn24.png" alt="">本身也是应用了链式规则的，例如<img src="../img/rnn25.png" alt="">。同样注意到，因为我们求的是向量的函数对向量的微分，结果是一个矩阵（称为雅可比矩阵），其中每个元素是对应的微分。我们可以重写以上的梯度：</p>
<p><img src="../img/rnn27.png" alt=""></p>
<p>It turns out (I won’t prove it here but this paper goes into detail) that the 2-norm, which you can think of it as an absolute value, of the above Jacobian matrix has an upper bound of 1. This makes intuitive sense because our tanh (or sigmoid) activation function maps all values into a range between -1 and 1, and the derivative is bounded by 1 (1/4 in the case of sigmoid) as well:</p>
<p><img src="../img/rnn28.png" alt=""></p>
<p>你可以看到tanh 和 sigmoid 函数在两端都有趋向0的导数。当这个发生时，我们说相应的神经元饱和（saturated）了。它们的梯度为0，并且导致在之前的其他层的梯度也变为0。因此，矩阵中的元素值很小加上多次的矩阵相乘，梯度值呈指数式收缩，最终在几个时间步之后完全消失了。在好几步时间步以前的梯度变成了0，而且这些时间步的状态不能给学习提供信息：结果就是学习不到长距离的依赖。消失的梯度不是RNN特有的。它同样会在deep Feedforward Neural Networks中发生。只是RNN很容易变的很深（在我们的例子中是和句子的长度一样深），这导致了消失的梯度问题更加常见。<br>不难想象，根据我们的激励函数和网络参数，当雅可比矩阵的元素值很大时，我们会的到一个爆炸式而不是消失的梯度。确实，这称为爆炸的梯度问题。之所以消失的梯度问题会获得更多的关注，有两方面原因：一、爆炸的梯度问题很明显。你的梯度会变成NaN，你的程序会crash。二、用一个预定的阈值限制梯度，是一个简单而有效的方法去防止这个爆炸的梯度问题。消失的梯度麻烦更大，因为它不明显，而且当发生的时候不容易处理。<br>幸运的是，现存很多方法去解决这个问题。合理地初始化W矩阵能减轻消失的梯度问题的影响，正则化也可以。更好的解决方法是使用<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks" target="_blank" rel="external">ReLU</a>)来代替tanh或者sigmoid激励函数。ReLU的微分要么是0要么是1，所以不太可能受到消失的梯度的影响。更加流行的方法是使用Long Short-Term Memory(LSTM) 或者 Gated Recurrent Unit（GRU）架构。LSTMs were <a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">first proposed in 1997</a> and are the perhaps most widely used models in NLP today. GRUs, <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">first proposed in 2014</a>, are simplified versions of LSTMs. 这两种RNN架构都是为了解决消失的梯度问题并且有效地学习长距离依赖而设计的。我们会在下部分的教程中讨论它们。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/27/RNN教程，part3，BPTT and Vanishing Gradients/" data-id="ciusb88yr000zbpplbxkbpda6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2016/10/24/RNN教程，part1-介绍RNN（译）/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/MarkDown-语法/">MarkDown 语法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/React-todoList/">React todoList</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/css-三栏布局-自适应/">css 三栏布局 自适应</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/es6-笔记/">es6 笔记</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/javascript-效率/">javascript 效率</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mvc-javascript/">mvc javascript</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nodejs-express-mongodb-入门/">nodejs express mongodb 入门</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/学习笔记/">学习笔记</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/MarkDown-语法/" style="font-size: 10px;">MarkDown 语法</a> <a href="/tags/React-todoList/" style="font-size: 10px;">React todoList</a> <a href="/tags/css-三栏布局-自适应/" style="font-size: 10px;">css 三栏布局 自适应</a> <a href="/tags/es6-笔记/" style="font-size: 10px;">es6 笔记</a> <a href="/tags/javascript-效率/" style="font-size: 10px;">javascript 效率</a> <a href="/tags/mvc-javascript/" style="font-size: 10px;">mvc javascript</a> <a href="/tags/nodejs-express-mongodb-入门/" style="font-size: 10px;">nodejs express mongodb 入门</a> <a href="/tags/学习笔记/" style="font-size: 10px;">学习笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">18</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/10/27/RNN教程，part3，BPTT and Vanishing Gradients/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/10/24/RNN教程，part1-介绍RNN（译）/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/10/24/RNN教程,part4,实现GRU:LSTM RNN/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/10/24/RNN教程,part2-实现一个RNN/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/09/21/redux入门/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Jennings<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>