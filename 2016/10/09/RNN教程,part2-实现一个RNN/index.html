<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>jenningL</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="这是RNN教程的第二部分。
github
这个部分我们会从头开始实现一个完全的递归神经网络，使用的是python，以及Theano来优化我们实现（这是一个执行GPU操作的库）。我会跳过一些样板代码，因为这对理解RNN没有什么帮助，但还是可以在github上找到它们。
语言建模我们的目标是使用RNN建立一个语言模型。假设我们有m个单词的句子，一个语言模型可以让我去预测观测到这个句子的概率（在一个给定">
<meta property="og:type" content="article">
<meta property="og:title" content="jenningL">
<meta property="og:url" content="http://yoursite.com/2016/10/09/RNN教程,part2-实现一个RNN/index.html">
<meta property="og:site_name" content="jenningL">
<meta property="og:description" content="这是RNN教程的第二部分。
github
这个部分我们会从头开始实现一个完全的递归神经网络，使用的是python，以及Theano来优化我们实现（这是一个执行GPU操作的库）。我会跳过一些样板代码，因为这对理解RNN没有什么帮助，但还是可以在github上找到它们。
语言建模我们的目标是使用RNN建立一个语言模型。假设我们有m个单词的句子，一个语言模型可以让我去预测观测到这个句子的概率（在一个给定">
<meta property="og:image" content="http://yoursite.com/../img/rnn1.png">
<meta property="og:image" content="http://yoursite.com/../imgs/rnn2.png">
<meta property="og:image" content="http://yoursite.com/../imgs/rnn3.png">
<meta property="og:image" content="http://yoursite.com/../imgs/rnn4.png">
<meta property="og:image" content="http://yoursite.com/../imgs/rnn5.png">
<meta property="og:image" content="http://yoursite.com/../imgs/rnn6.png">
<meta property="og:image" content="http://yoursite.com/../imgs/rnn7.png">
<meta property="og:image" content="http://yoursite.com/../imgs/rnn8.png">
<meta property="og:image" content="http://yoursite.com/../imgs/rnn8.png">
<meta property="og:image" content="http://yoursite.com/../imgs/rnn9.png">
<meta property="og:updated_time" content="2016-10-09T12:58:41.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="jenningL">
<meta name="twitter:description" content="这是RNN教程的第二部分。
github
这个部分我们会从头开始实现一个完全的递归神经网络，使用的是python，以及Theano来优化我们实现（这是一个执行GPU操作的库）。我会跳过一些样板代码，因为这对理解RNN没有什么帮助，但还是可以在github上找到它们。
语言建模我们的目标是使用RNN建立一个语言模型。假设我们有m个单词的句子，一个语言模型可以让我去预测观测到这个句子的概率（在一个给定">
  
    <link rel="alternative" href="/atom.xml" title="jenningL" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">jenningL</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-RNN教程,part2-实现一个RNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/09/RNN教程,part2-实现一个RNN/" class="article-date">
  <time datetime="2016-10-09T12:58:41.000Z" itemprop="datePublished">2016-10-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这是RNN教程的第二部分。</p>
<p><a href="https://github.com/dennybritz/rnn-tutorial-rnnlm" target="_blank" rel="external">github</a></p>
<p>这个部分我们会从头开始实现一个完全的递归神经网络，使用的是python，以及Theano来优化我们实现（这是一个执行GPU操作的库）。我会跳过一些样板代码，因为这对理解RNN没有什么帮助，但还是可以在github上找到它们。</p>
<h2 id="语言建模">语言建模</h2><p>我们的目标是使用RNN建立一个语言模型。假设我们有m个单词的句子，一个语言模型可以让我去预测观测到这个句子的概率（在一个给定的数据集中）：<br><img src="../img/rnn1.png" alt="概率公式"><br>句子的概率是(在给定前面出现的单词序列的条件下每个单词出现的概率)的乘积。所以，”He went to buy some chocolate” 这个句子的概率是”chocolate”出现在”He went to buy some”后面的概率乘以”some”出现在”He went to buy”后面的概率…以此类推。<br>这有什么用？为什么我们会想要计算一个句子的被观测到的概率？<br>首先，这样一个模型可以被用在打分机制中。例如，一个机器翻译系统通常会产生一些候选翻译结果，你可以使用这个模型去挑选一个最有可能的结果。从直觉上来说，概率最大的句子很有可能在语法上是正确的。在语音识别系统中也存在类似的打分。<br>但得到语言模型的同时也有一个很酷的副产物，因为我们可以预测给定前缀条件下一个单词出现的概率。我们可以产生新的文本。这是一个生产模型。给定一个现存的单词序列，我们根据预测概率选取最有可能的下一个单词，如此反复直到得到一个完整的句子。 关于这个，Andrej Karparthy写了一篇<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">很不错的文章</a>。他的模型是按字母粒度训练而不是按单词，可以产生从莎士比亚文集到Linux代码的任何东西。<br>注意到上面的等式，每个单词的出现概率取决于所有之前的单词。实际上，许多模型都因为计算复杂度和内存的限制很难表示这么长的依赖。它们通常会限制往回看的单词个数。RNN理论上可以捕获这么长的依赖，但是在实际上会更复杂些，我们以后会再讨论这个问题。</p>
<h2 id="训练数据和预处理">训练数据和预处理</h2><p>为了训练我们的语言模型，我们需要文本来学习。幸运的是，我们不需要标注，只需要原文本。我从<a href="https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_08" target="_blank" rel="external">Google’s BigQuery</a>下载了15000稍长的reddit评论。我们的模型产生的文本会看起来像reddit评论，希望如此。但正如大多数机器学习的项目，我们首先需要做一些预处理，把我们的数据转换为正确的格式。</p>
<h3 id="1-分词">1.分词</h3><p>我们有原始的文本，但我们想要基于单词粒度去做预测。这意味着我们必须将评论划分为句子，然后再划分为单词。我们可以简单地按照空格划分，但是这样不能正确处理标点符号。我们将使用<a href="http://www.nltk.org/" target="_blank" rel="external">NLTK</a>的word_tokenize和sent_tokenize 方法，它们将帮我们解决这个问题。</p>
<h3 id="2-移除罕见的单词">2.移除罕见的单词</h3><p>大多数单词在我们的文本中只会出现一到两次。移除这些罕见单词是个好主意。词汇量太大会增加训练的时间，而且我们没有很多这些单词上下文相关的例子，所以我们没办法学习到怎么正确地使用它们。这跟人类的学习方式是类似的。为了学习一个单词的使用方法，你必须在多个场景中见到它。<br>在我们的代码中，我限制我们的词汇为最常见的单词（8000）。我们将所有不在词汇里面的单词替换为UNKNOWN_TOKEN。例如，如果我们的词汇里没有包含”nonlinearities”这个单词，那么“nonlineraties are important in neural networks” 将变成 “UNKNOWN_TOKEN are important in Neural Networks”。UNKNOWN_TOKEN会成为我们词汇的一部分，我们会像其他单词一样预测它。当我们产生新的文本时，我们可以再次替换UNKNOWN_TOKEN。例如随机选取一个不在我们的词汇中的词，或者我们重新产生句子直到我们没有出现UNKNOWN_TOKEN为止。</p>
<h3 id="3-加入特殊的开始符和结束符">3.加入特殊的开始符和结束符</h3><p>我们同样希望学习到那些单词倾向于开始或者结束一个句子。为了做到这个，我们在句子前面加入了一个特殊的SENTENCE_START符号，在句子后面加入SENTENCE_END符号。这允许我们问：当出现SENTENCE_START时，下一个最可能出现的单词是什么？（句子中的第一个单词）</p>
<h3 id="4-建立训练数据矩阵">4.建立训练数据矩阵</h3><p>RNN的输入是向量，不是字符串。所以我们创建了单词和索引之间的映射:index_to_word，word_to_index。例如， 单词“friendly”可能在2001的下标，一个训练样本x可能像这样[0, 179, 341, 416]，其中0对应了SENTENCE_START。对应的标签y应该为[179, 341, 416, 1]。记住我们的目标是预测下一个单词，所以y只是x向量向左移一位，最后一个符号是SENTENCE_END。换句话说，单词179的预测应该是341（真实出现的下一个单词）。<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">vocabulary_size = <span class="number">8000</span></span><br><span class="line">unknown_token = <span class="string">"UNKNOWN_TOKEN"</span></span><br><span class="line">sentence_start_token = <span class="string">"SENTENCE_START"</span></span><br><span class="line">sentence_end_token = <span class="string">"SENTENCE_END"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the data and append SENTENCE_START and SENTENCE_END tokens</span></span><br><span class="line">print <span class="string">"Reading CSV file..."</span></span><br><span class="line"><span class="operator">with</span> <span class="built_in">open</span>(<span class="string">'data/reddit-comments-2015-08.csv'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    reader = csv.reader(f, skipinitialspace=True)</span><br><span class="line">    reader.next()</span><br><span class="line">    <span class="comment"># Split full comments into sentences</span></span><br><span class="line">    <span class="keyword">sentences</span> = itertools.chain(*[nltk.sent_tokenize(x[<span class="number">0</span>].decode(<span class="string">'utf-8'</span>).<span class="built_in">lower</span>()) <span class="keyword">for</span> x <span class="operator">in</span> reader])</span><br><span class="line">    <span class="comment"># Append SENTENCE_START and SENTENCE_END</span></span><br><span class="line">    <span class="keyword">sentences</span> = [<span class="string">"%s %s %s"</span> % (sentence_start_token, x, sentence_end_token) <span class="keyword">for</span> x <span class="operator">in</span> <span class="keyword">sentences</span>]</span><br><span class="line">print <span class="string">"Parsed %d sentences."</span> % (<span class="built_in">len</span>(<span class="keyword">sentences</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenize the sentences into words</span></span><br><span class="line">tokenized_sentences = [nltk.word_tokenize(sent) <span class="keyword">for</span> sent <span class="operator">in</span> <span class="keyword">sentences</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Count the word frequencies</span></span><br><span class="line">word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))</span><br><span class="line">print <span class="string">"Found %d unique words tokens."</span> % <span class="built_in">len</span>(word_freq.<span class="keyword">items</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the most common words and build index_to_word and word_to_index vectors</span></span><br><span class="line">vocab = word_freq.most_common(vocabulary_size-<span class="number">1</span>)</span><br><span class="line">index_to_word = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="operator">in</span> vocab]</span><br><span class="line">index_to_word.append(unknown_token)</span><br><span class="line">word_to_index = dict([(w,i) <span class="keyword">for</span> i,w <span class="operator">in</span> enumerate(index_to_word)])</span><br><span class="line"></span><br><span class="line">print <span class="string">"Using vocabulary size %d."</span> % vocabulary_size</span><br><span class="line">print <span class="string">"The least frequent word in our vocabulary is '%s' and appeared %d times."</span> % (vocab[-<span class="number">1</span>][<span class="number">0</span>], vocab[-<span class="number">1</span>][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace all words not in our vocabulary with the unknown token</span></span><br><span class="line"><span class="keyword">for</span> i, sent <span class="operator">in</span> enumerate(tokenized_sentences):</span><br><span class="line">    tokenized_sentences[i] = [w <span class="keyword">if</span> w <span class="operator">in</span> word_to_index <span class="keyword">else</span> unknown_token <span class="keyword">for</span> w <span class="operator">in</span> sent]</span><br><span class="line"></span><br><span class="line">print <span class="string">"\nExample sentence: '%s'"</span> % <span class="keyword">sentences</span>[<span class="number">0</span>]</span><br><span class="line">print <span class="string">"\nExample sentence after Pre-processing: '%s'"</span> % tokenized_sentences[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the training data</span></span><br><span class="line">X_train = np.asarray([[word_to_index[w] <span class="keyword">for</span> w <span class="operator">in</span> sent[:-<span class="number">1</span>]] <span class="keyword">for</span> sent <span class="operator">in</span> tokenized_sentences])</span><br><span class="line">y_train = np.asarray([[word_to_index[w] <span class="keyword">for</span> w <span class="operator">in</span> sent[<span class="number">1</span>:]] <span class="keyword">for</span> sent <span class="operator">in</span> tokenized_sentences])</span><br></pre></td></tr></table></figure></p>
<p>这是我们文本的真实训练样例：<br><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x:</span><br><span class="line">SENTENCE_START what <span class="keyword">are</span> n't you understanding about this ? !</span><br><span class="line"><span class="comment">[0, 51, 27, 16, 10, 856, 53, 25, 34, 69]</span></span><br><span class="line"></span><br><span class="line">y:</span><br><span class="line">what <span class="keyword">are</span> n't you understanding about this ? ! SENTENCE_END</span><br><span class="line"><span class="comment">[51, 27, 16, 10, 856, 53, 25, 34, 69, 1]</span></span><br></pre></td></tr></table></figure></p>
<h3 id="建造RNN">建造RNN</h3><p><img src="../imgs/rnn2.png" alt="overview"><br>接下来讲讲RNN的具体细节。输入x是单词的序列（就像上面打印的一样），每个xt是一个单词。但还有一件事：因为矩阵相乘的原因，我们不能简单地使用单词的索引（像36）作为输入，而是用vocabulary_size大小的one-hot vector来表示每个单词。例如下标是36的单词就是除了第36个元素是1外其他都是0。所以，每个xt是一个向量，x是一个矩阵，其中每一行代表一个单词。我们会在神经网络的代码中做这个转换而不是在预处理中。我们网络的输出o也是类似的格式，每个ot是一个vocabulary_size大小的向量，每个元素代表了对应下标的单词作为下一个单词出现的概率。<br>让我们来回顾以下RNN的计算公式：<br><img src="../imgs/rnn3.png" alt="equations"><br>我发现写下矩阵和向量的维度总是很有用的。让我们假设词汇库的大小C=8000，隐藏层的大小H=100。你可以认为隐藏层的大小是网络的记忆。更大隐藏层可以学习到更复杂的模式，但同时会增加计算量。我们有：<br><img src="../imgs/rnn4.png" alt="size"><br>这是很有用的信息。记住U，V和W是我们需要从数据中学习到的参数。因此我们总共需要学习2HC+H^2个参数。当C=8000，H=100时，这个数字是1610000。这个维度告诉我们模型的瓶颈。注意到xt是one-hot vector，用它和U相乘本质上是选择U中的一列，所以我们不需要执行完整的乘法。然后，最大的矩阵相乘是Vst，这也是我们希望尽量控制词汇库大小的原因。<br>有了以上的知识，我们开始实现这个模型。</p>
<h2 id="初始化">初始化</h2><p>我们从声明一个RNN类以及初始化参数开始。我将这个类称为RNNNumpy，因为之后会实现一个Theano版本的。初始化U，V和W需要一些技巧。我们不能将它们初始化为全0，因为那会导致在全部层中的对称计算。我们必须随机初始化它们。因为适当的初始化看起来对训练结果有很大的影响，所以很多这方面的研究。结果显示，最好的初始化方式是基于activation function（tanh in our case），而一个推荐的方法是在区间<img src="../imgs/rnn5.png" alt="interval">随机初始化这些权重，where n is the number of incoming connections from the previous layer（看不太懂，所以不翻了）。<br><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNNumpy</span>:</span></span><br><span class="line"></span><br><span class="line">    def __init__(<span class="variable">self</span>, word_dim, hidden_dim=<span class="number">100</span>, bptt_truncate=<span class="number">4</span>):<span class="preprocessor"></span><br><span class="line">        # Assign instance variables</span></span><br><span class="line">        <span class="variable">self</span>.word_dim = word_dim</span><br><span class="line">        <span class="variable">self</span>.hidden_dim = hidden_dim</span><br><span class="line">        <span class="variable">self</span>.bptt_truncate = bptt_truncate<span class="preprocessor"></span><br><span class="line">        # Randomly initialize the network parameters</span></span><br><span class="line">        <span class="variable">self</span>.U = np.random.uniform(-np.<span class="built_in">sqrt</span>(<span class="number">1</span>./word_dim), np.<span class="built_in">sqrt</span>(<span class="number">1</span>./word_dim), (hidden_dim, word_dim))</span><br><span class="line">        <span class="variable">self</span>.V = np.random.uniform(-np.<span class="built_in">sqrt</span>(<span class="number">1</span>./hidden_dim), np.<span class="built_in">sqrt</span>(<span class="number">1</span>./hidden_dim), (word_dim, hidden_dim))</span><br><span class="line">        <span class="variable">self</span>.W = np.random.uniform(-np.<span class="built_in">sqrt</span>(<span class="number">1</span>./hidden_dim), np.<span class="built_in">sqrt</span>(<span class="number">1</span>./hidden_dim), (hidden_dim, hidden_dim))</span><br></pre></td></tr></table></figure></p>
<p>在上面的代码中，word_dim是我们词汇库的大小， hidden_dim是隐藏层的大小，bptt_truncate将在之后解释。</p>
<h2 id="前向传播">前向传播</h2><p>现在我们来实现前向传播（预测单词的概率），根据计算公式的定义：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="comment"># The total number of time steps</span></span><br><span class="line">    T = len(x)</span><br><span class="line">    <span class="comment"># During forward propagation we save all hidden states in s because need them later.</span></span><br><span class="line">    <span class="comment"># We add one additional element for the initial hidden, which we set to 0</span></span><br><span class="line">    s = np.zeros((T + <span class="number">1</span>, self.hidden_dim))</span><br><span class="line">    s[-<span class="number">1</span>] = np.zeros(self.hidden_dim)</span><br><span class="line">    <span class="comment"># The outputs at each time step. Again, we save them for later.</span></span><br><span class="line">    o = np.zeros((T, self.word_dim))</span><br><span class="line">    <span class="comment"># For each time step...</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(T):</span><br><span class="line">        <span class="comment"># <span class="doctag">Note</span> that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.</span></span><br><span class="line">        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-<span class="number">1</span>]))</span><br><span class="line">        o[t] = softmax(self.V.dot(s[t]))</span><br><span class="line">    <span class="keyword">return</span> [o, s]</span><br><span class="line"></span><br><span class="line">RNNNumpy.forward_propagation = forward_propagation</span><br></pre></td></tr></table></figure></p>
<p>我们同时返回了输出o和隐藏状态s，它们之后会被用于计算梯度。但有时候，我们只需要最高概率的下一个单词，因此我们需要一个predict函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="comment"># Perform forward propagation and return index of the highest score</span></span><br><span class="line">    o, s = self.forward_propagation(x)</span><br><span class="line">    <span class="keyword">return</span> np.argmax(o, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">RNNNumpy.predict = predict</span><br></pre></td></tr></table></figure></p>
<p>我们来试一下这两个方法以及它们的输出：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">np<span class="class">.random</span><span class="class">.seed</span>(<span class="number">10</span>)</span><br><span class="line">model = <span class="function"><span class="title">RNNNumpy</span><span class="params">(vocabulary_size)</span></span></span><br><span class="line">o, s = model.<span class="function"><span class="title">forward_propagation</span><span class="params">(X_train[<span class="number">10</span>])</span></span></span><br><span class="line">print o<span class="class">.shape</span></span><br><span class="line">print o</span><br><span class="line"></span><br><span class="line">predictions = model.<span class="function"><span class="title">predict</span><span class="params">(X_train[<span class="number">10</span>])</span></span></span><br><span class="line">print predictions<span class="class">.shape</span></span><br><span class="line">print predictions</span><br></pre></td></tr></table></figure></p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(45, 8000)</span><br><span class="line">[[ <span class="number">0.00012408</span>  <span class="number">0.0001244</span>   <span class="number">0.00012603</span> ...,  <span class="number">0.00012515</span>  <span class="number">0.00012488</span></span><br><span class="line">   <span class="number">0.00012508</span>]</span><br><span class="line"> [ <span class="number">0.00012536</span>  <span class="number">0.00012582</span>  <span class="number">0.00012436</span> ...,  <span class="number">0.00012482</span>  <span class="number">0.00012456</span></span><br><span class="line">   <span class="number">0.00012451</span>]</span><br><span class="line"> [ <span class="number">0.00012387</span>  <span class="number">0.0001252</span>   <span class="number">0.00012474</span> ...,  <span class="number">0.00012559</span>  <span class="number">0.00012588</span></span><br><span class="line">   <span class="number">0.00012551</span>]</span><br><span class="line"> ...,</span><br><span class="line"> [ <span class="number">0.00012414</span>  <span class="number">0.00012455</span>  <span class="number">0.0001252</span>  ...,  <span class="number">0.00012487</span>  <span class="number">0.00012494</span></span><br><span class="line">   <span class="number">0.0001263</span> ]</span><br><span class="line"> [ <span class="number">0.0001252</span>   <span class="number">0.00012393</span>  <span class="number">0.00012509</span> ...,  <span class="number">0.00012407</span>  <span class="number">0.00012578</span></span><br><span class="line">   <span class="number">0.00012502</span>]</span><br><span class="line"> [ <span class="number">0.00012472</span>  <span class="number">0.0001253</span>   <span class="number">0.00012487</span> ...,  <span class="number">0.00012463</span>  <span class="number">0.00012536</span></span><br><span class="line">   <span class="number">0.00012665</span>]]</span><br><span class="line"></span><br><span class="line">   (45,)</span><br><span class="line">[<span class="number">1284 5221</span> <span class="number">7653 7430</span> <span class="number">1013 3562</span> <span class="number">7366 4860</span> <span class="number">2212 6601</span> <span class="number">7299 4556</span> <span class="number">2481 238 253</span>9</span><br><span class="line"> <span class="number">21 6548 26</span><span class="number">1 1780 200</span><span class="number">5 1810 53</span><span class="number">76 4146 47</span><span class="number">7 7051 48</span><span class="number">32 4991 89</span><span class="number">7 3485 21</span></span><br><span class="line"> <span class="number">7291 2007</span> <span class="number">6006 760</span> <span class="number">4864 2182</span> <span class="number">6569 2800</span> <span class="number">2752 6821</span> <span class="number">4437 7021</span> <span class="number">7875 6912</span> 3575]</span><br></pre></td></tr></table></figure>
<p>由于我们的随机初始化了U，V，W，所以现在预测是完全随机的。</p>
<h2 id="计算代价">计算代价</h2><p>我们使用代价函数L来衡量模型的表现，我们的目标是找到参数U，V，W，使得代价函数在我们的数据里最小。一个常见的选择是cross-entropy loss。如果我们有N个训练样本，和C个类别（词汇库的大小），那么预测o和标签y的代价函数是：<br><img src="../imgs/rnn6.png" alt="cost-function"><br>根据公式的定义，假设标签说第i个位置是正确的答案，也就是说y_n的第i个元素是1时，o_n的第i个元素如果为1，那么yn<em>logo_n=0，因为正确所以代价为0；反之，如果o_n的第i个元素比1小，那么yn</em>logo_n &lt; 0，所以代价不为0，而且o_n偏离1越远，代价越大。可以看到下面的实现中，因为y_n实际上是one-hot vector，所以我们只需要看正确的位置上对应的o_n即可，因为其他代价肯定是0。<br>以下是实现代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_total_loss</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    L = <span class="number">0</span></span><br><span class="line">    <span class="comment"># For each sentence...</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(len(y)):</span><br><span class="line">        o, s = self.forward_propagation(x[i])</span><br><span class="line">        <span class="comment"># We only care about our prediction of the "correct" words</span></span><br><span class="line">        correct_word_predictions = o[np.arange(len(y[i])), y[i]]</span><br><span class="line">        <span class="comment"># Add to the loss based on how off we were</span></span><br><span class="line">        L += -<span class="number">1</span> * np.sum(np.log(correct_word_predictions))</span><br><span class="line">    <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_loss</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    <span class="comment"># Divide the total loss by the number of training examples</span></span><br><span class="line">    N = np.sum((len(y_i) <span class="keyword">for</span> y_i <span class="keyword">in</span> y))</span><br><span class="line">    <span class="keyword">return</span> self.calculate_total_loss(x,y)/N</span><br><span class="line"></span><br><span class="line">RNNNumpy.calculate_total_loss = calculate_total_loss</span><br><span class="line">RNNNumpy.calculate_loss = calculate_loss</span><br></pre></td></tr></table></figure></p>
<p>让我们后退一步，思考一下随机预测的代价应该是多少。这会给我们一个参考基线，以及确保我们的实现是正确的。我们的词汇库大小是C，因此每个单词应该以1/C的概率被预测，进而得到代价是：<br><img src="../imgs/rnn7.png" alt="loss"><br>可以通过实验验证：<br><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Limit to 1000 examples to save time</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Expected Loss for random predictions: <span class="variable">%f</span>"</span> % np.<span class="keyword">log</span>(vocabulary_size)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Actual loss: <span class="variable">%f</span>"</span> % model.calculate_loss(X_train[:<span class="number">1000</span>], y_train[:<span class="number">1000</span>])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Expected Loss <span class="keyword">for</span> random <span class="string">predictions:</span> <span class="number">8.987197</span></span><br><span class="line">Actual <span class="string">loss:</span> <span class="number">8.987440</span></span><br></pre></td></tr></table></figure>
<h2 id="使用SGD和BPTT来训练RNN">使用SGD和BPTT来训练RNN</h2><p>记住我们的目标是寻找参数U，V，W使得在我们的数据中总代价最小。最常见的方法是随机梯度下降Stochastic Gradient Descent（SGD）。SGD背后的思想很简单。我们在每轮迭代中，遍历所有的训练样本并将参数推向一个使误差减少的方向。这个方向由误差（代价）的梯度<img src="../imgs/rnn8.png" alt="">来给出。SGD同样需要一个学习效率（learning rate），也就是每次迭代的步长。关于SGD的教程很多，这里不再赘述。<a href="http://cs231n.github.io/optimization-1/" target="_blank" rel="external">这里有个教程</a>。我将实现一个简单版本的SGD，即使是没有相关优化背景的人也能看懂。<br>但我们怎么计算上面提到的梯度？在传统神经网络中，我们使用反向传播算法来计算。在RNN中，我们使用一个稍微不同的算法叫Backpropagation Through Time(BPTT)。因为参数在所有时间步中被共享，每个输出的梯度不仅依赖于当前时间步，还依赖于之前的时间步。如果你懂微积分，这真的只是应用chain rule。下一部分教程全都是关于BPTT的，所以现在我们只需要将它当作一个黑盒子，接收训练样本(x, y)作为输入，返回梯度<img src="../imgs/rnn8.png" alt=""><br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bptt</span></span>(<span class="keyword">self</span>, x, y)<span class="symbol">:</span></span><br><span class="line">    <span class="constant">T </span>= len(y)</span><br><span class="line">    <span class="comment"># Perform forward propagation</span></span><br><span class="line">    o, s = <span class="keyword">self</span>.forward_propagation(x)</span><br><span class="line">    <span class="comment"># We accumulate the gradients in these variables</span></span><br><span class="line">    dLdU = np.zeros(<span class="keyword">self</span>.<span class="constant">U.</span>shape)</span><br><span class="line">    dLdV = np.zeros(<span class="keyword">self</span>.<span class="constant">V.</span>shape)</span><br><span class="line">    dLdW = np.zeros(<span class="keyword">self</span>.<span class="constant">W.</span>shape)</span><br><span class="line">    delta_o = o</span><br><span class="line">    delta_o[np.arange(len(y)), y] -= <span class="number">1</span>.</span><br><span class="line">    <span class="comment"># For each output backwards...</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(<span class="constant">T)</span>[<span class="symbol">:</span><span class="symbol">:-</span><span class="number">1</span>]<span class="symbol">:</span></span><br><span class="line">        dLdV += np.outer(delta_o[t], s[t].<span class="constant">T)</span></span><br><span class="line">        <span class="comment"># Initial delta calculation</span></span><br><span class="line">        delta_t = <span class="keyword">self</span>.<span class="constant">V.T.</span>dot(delta_o[t]) * (<span class="number">1</span> - (s[t] ** <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># Backpropagation through time (for at most self.bptt_truncate steps)</span></span><br><span class="line">        <span class="keyword">for</span> bptt_step <span class="keyword">in</span> np.arange(max(<span class="number">0</span>, t-<span class="keyword">self</span>.bptt_truncate), t+<span class="number">1</span>)[<span class="symbol">:</span><span class="symbol">:-</span><span class="number">1</span>]<span class="symbol">:</span></span><br><span class="line">            <span class="comment"># print "Backpropagation step t=%d bptt step=%d " % (t, bptt_step)</span></span><br><span class="line">            dLdW += np.outer(delta_t, s[bptt_step-<span class="number">1</span>])              </span><br><span class="line">            dLdU[<span class="symbol">:</span>,x[bptt_step]] += delta_t</span><br><span class="line">            <span class="comment"># Update delta for next step</span></span><br><span class="line">            delta_t = <span class="keyword">self</span>.<span class="constant">W.T.</span>dot(delta_t) * (<span class="number">1</span> - s[bptt_step-<span class="number">1</span>] ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> [dLdU, dLdV, dLdW]</span><br><span class="line"></span><br><span class="line"><span class="constant">RNNNumpy.</span>bptt = bptt</span><br></pre></td></tr></table></figure></p>
<h2 id="检查梯度">检查梯度</h2><p>在实现实现反向传播的同时实现梯度检查是一个好主意，它能够验证你的实现是否正确。梯度检查背后的思想是参数的微分等于该点的斜率，我们可以稍微改变参数然后除以这个改变，然后就可以得到微分的近似值：<br><img src="../imgs/rnn9.png" alt=""><br>然后我们使用反向传播计算的梯度和上面的方法计算的梯度进行比较，如果差别不大那就没问题。近似值需要计算每个参数的总代价，所以梯度检查是非常昂贵的操作（毕竟我们有超过1百万的参数需要学习）。所以让它在词汇库更小的模型中执行比较好：<br><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def gradient_check(self, <span class="keyword">x</span>, <span class="keyword">y</span>, h=<span class="number">0</span>.<span class="number">001</span>, error_threshold=<span class="number">0</span>.<span class="number">01</span>):</span><br><span class="line">    <span class="comment"># Calculate the gradients using backpropagation. We want to checker if these are correct.</span></span><br><span class="line">    bptt_gradients = self.bptt(<span class="keyword">x</span>, <span class="keyword">y</span>)</span><br><span class="line">    <span class="comment"># List of all parameters we want to check.</span></span><br><span class="line">    model_parameters = [<span class="string">'U'</span>, <span class="string">'V'</span>, <span class="string">'W'</span>]</span><br><span class="line">    <span class="comment"># Gradient check for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> pidx, pname in enumerate(model_parameters):</span><br><span class="line">        <span class="comment"># Get the actual parameter value from the mode, e.g. model.W</span></span><br><span class="line">        parameter = operator.attrgetter(pname)(self)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Performing gradient check for parameter <span class="variable">%s</span> with size <span class="variable">%d</span>."</span> % (pname, np.prod(parameter.shape))</span><br><span class="line">        <span class="comment"># Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...</span></span><br><span class="line">        it = np.nditer(parameter, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line">            ix = it.multi_index</span><br><span class="line">            <span class="comment"># Save the original value so we can reset it later</span></span><br><span class="line">            original_value = parameter[ix]</span><br><span class="line">            <span class="comment"># Estimate the gradient using (f(x+h) - f(x-h))/(2*h)</span></span><br><span class="line">            parameter[ix] = original_value + h</span><br><span class="line">            gradplus = self.calculate_total_loss([<span class="keyword">x</span>],[<span class="keyword">y</span>])</span><br><span class="line">            parameter[ix] = original_value - h</span><br><span class="line">            gradminus = self.calculate_total_loss([<span class="keyword">x</span>],[<span class="keyword">y</span>])</span><br><span class="line">            estimated_gradient = (gradplus - gradminus)/(<span class="number">2</span>*h)</span><br><span class="line">            <span class="comment"># Reset parameter to original value</span></span><br><span class="line">            parameter[ix] = original_value</span><br><span class="line">            <span class="comment"># The gradient for this parameter calculated using backpropagation</span></span><br><span class="line">            backprop_gradient = bptt_gradients[pidx][ix]</span><br><span class="line">            <span class="comment"># calculate The relative error: (|x - y|/(|x| + |y|))</span></span><br><span class="line">            relative_error = np.<span class="keyword">abs</span>(backprop_gradient - estimated_gradient)/(np.<span class="keyword">abs</span>(backprop_gradient) + np.<span class="keyword">abs</span>(estimated_gradient))</span><br><span class="line">            <span class="comment"># If the error is to large fail the gradient check</span></span><br><span class="line">            <span class="keyword">if</span> relative_error &amp;<span class="keyword">gt</span>; error_threshold:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Gradient Check ERROR: parameter=<span class="variable">%s</span> ix=<span class="variable">%s</span>"</span> % (pname, ix)</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"+h Loss: <span class="variable">%f</span>"</span> % gradplus</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"-h Loss: <span class="variable">%f</span>"</span> % gradminus</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Estimated_gradient: <span class="variable">%f</span>"</span> % estimated_gradient</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Backpropagation gradient: <span class="variable">%f</span>"</span> % backprop_gradient</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Relative Error: <span class="variable">%f</span>"</span> % relative_error</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            it.iternext()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Gradient check for parameter <span class="variable">%s</span> passed."</span> % (pname)</span><br><span class="line"></span><br><span class="line">RNNNumpy.gradient_check = gradient_check</span><br><span class="line"></span><br><span class="line"><span class="comment"># To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.</span></span><br><span class="line">grad_check_vocab_size = <span class="number">100</span></span><br><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line">model = RNNNumpy(grad_check_vocab_size, <span class="number">10</span>, bptt_truncate=<span class="number">1000</span>)</span><br><span class="line">model.gradient_check([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure></p>
<h2 id="实现SGD">实现SGD</h2><p>既然我们已经可以计算参数的梯度了，那我们就可以实现SGD了。我将它分为两个步骤：</p>
<ol>
<li>一个函数sdg_step，用来计算一份数据的梯度然后执行更新</li>
<li>外层循环，用以遍历训练集，然后调整学习率<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Performs one step of SGD.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numpy_sdg_step</span><span class="params">(self, x, y, learning_rate)</span>:</span></span><br><span class="line">    <span class="comment"># Calculate the gradients</span></span><br><span class="line">    dLdU, dLdV, dLdW = self.bptt(x, y)</span><br><span class="line">    <span class="comment"># Change parameters according to gradients and learning rate</span></span><br><span class="line">    self.U -= learning_rate * dLdU</span><br><span class="line">    self.V -= learning_rate * dLdV</span><br><span class="line">    self.W -= learning_rate * dLdW</span><br><span class="line"></span><br><span class="line">RNNNumpy.sgd_step = numpy_sdg_step</span><br></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Outer SGD Loop</span></span><br><span class="line"><span class="comment"># - model: The RNN model instance</span></span><br><span class="line"><span class="comment"># - X_train: The training data set</span></span><br><span class="line"><span class="comment"># - y_train: The training data labels</span></span><br><span class="line"><span class="comment"># - learning_rate: Initial learning rate for SGD</span></span><br><span class="line"><span class="comment"># - nepoch: Number of times to iterate through the complete dataset</span></span><br><span class="line"><span class="comment"># - evaluate_loss_after: Evaluate the loss after this many epochs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_with_sgd</span><span class="params">(model, X_train, y_train, learning_rate=<span class="number">0.005</span>, nepoch=<span class="number">100</span>, evaluate_loss_after=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="comment"># We keep track of the losses so we can plot them later</span></span><br><span class="line">    losses = []</span><br><span class="line">    num_examples_seen = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(nepoch):</span><br><span class="line">        <span class="comment"># Optionally evaluate the loss</span></span><br><span class="line">        <span class="keyword">if</span> (epoch % evaluate_loss_after == <span class="number">0</span>):</span><br><span class="line">            loss = model.calculate_loss(X_train, y_train)</span><br><span class="line">            losses.append((num_examples_seen, loss))</span><br><span class="line">            time = datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"%s: Loss after num_examples_seen=%d epoch=%d: %f"</span> % (time, num_examples_seen, epoch, loss)</span><br><span class="line">            <span class="comment"># Adjust the learning rate if loss increases</span></span><br><span class="line">            <span class="keyword">if</span> (len(losses) &amp;gt; <span class="number">1</span> <span class="keyword">and</span> losses[-<span class="number">1</span>][<span class="number">1</span>] &amp;gt; losses[-<span class="number">2</span>][<span class="number">1</span>]):</span><br><span class="line">                learning_rate = learning_rate * <span class="number">0.5</span></span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Setting learning rate to %f"</span> % learning_rate</span><br><span class="line">            sys.stdout.flush()</span><br><span class="line">        <span class="comment"># For each training example...</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_train)):</span><br><span class="line">            <span class="comment"># One SGD step</span></span><br><span class="line">            model.sgd_step(X_train[i], y_train[i], learning_rate)</span><br><span class="line">            num_examples_seen += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>现在让我们来测试一下需要多长的训练时间：<br><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed<span class="params">(<span class="number">10</span>)</span></span><br><span class="line">model = RNNNumpy<span class="params">(vocabulary_size)</span></span><br><span class="line"><span class="built_in">%</span>timeit model.sgd_step<span class="params">(X_train[<span class="number">10</span>], y_train[<span class="number">10</span>], <span class="number">0.005</span>)</span></span><br></pre></td></tr></table></figure></p>
<p>在作者的笔记本电脑上，一步SGD要花350ms，我们有80000个训练数据样本，那一次遍历就要几个小时，多次次遍历要花几天甚至几周！而且相比真正的研究人员我们使用的数据已经算小的了。那怎么办呢？<br>幸运的是，有很多方法可以加速我们的代码。我们可以继续使用同样的模型，而让代码跑的更快，或者我们可以修改模型使得计算复杂度下降，或者两者都有。研究人员已经提出了很多优化模型计算复杂的方法，例如，使用<a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="external">hierarchical</a> softmax 或者<a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf" target="_blank" rel="external">adding projection layers to avoid the large matrix multiplications</a>。但我想保持我们模型简单，所以选择了第一种方法：使用GPU加速我们的代码。让我们尝试使用小数据集来检查代价是否真的有在减少。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># Train on a small subset of the data to see what happens</span></span><br><span class="line"><span class="variable">model =</span> RNNNumpy(vocabulary_size)</span><br><span class="line"><span class="variable">losses =</span> train_with_sgd(model, X_train[:<span class="number">100</span>], y_train[:<span class="number">100</span>], <span class="variable">nepoch=</span><span class="number">10</span>, <span class="variable">evaluate_loss_after=</span><span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015-09-30</span> 10:08:19: Loss after num_examples_seen=0 epoch=0: <span class="number">8.987425</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:08:35: Loss after num_examples_seen=100 epoch=1: <span class="number">8.976270</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:08:50: Loss after num_examples_seen=200 epoch=2: <span class="number">8.960212</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:09:06: Loss after num_examples_seen=300 epoch=3: <span class="number">8.930430</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:09:22: Loss after num_examples_seen=400 epoch=4: <span class="number">8.862264</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:09:38: Loss after num_examples_seen=500 epoch=5: <span class="number">6.913570</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:09:53: Loss after num_examples_seen=600 epoch=6: <span class="number">6.302493</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:10:07: Loss after num_examples_seen=700 epoch=7: <span class="number">6.014995</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:10:24: Loss after num_examples_seen=800 epoch=8: <span class="number">5.833877</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:10:39: Loss after num_examples_seen=900 epoch=9: <span class="number">5.710718</span></span><br></pre></td></tr></table></figure>
<p>实验证明实现是符合预期的。</p>
<h2 id="使用Theano_和_GPU_训练我们的网络">使用Theano 和 GPU 训练我们的网络</h2><p>我之前写了一篇关于Theano的<a href="http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/" target="_blank" rel="external">教程</a>，鉴于所有的逻辑都是一样的，我不会在这里过一遍优化代码。我定义了一个RNNTheano类，用来取代原来的实现版本。代码同样能在<a href="https://github.com/dennybritz/rnn-tutorial-rnnlm" target="_blank" rel="external">github</a>上找到。<br><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed<span class="params">(<span class="number">10</span>)</span></span><br><span class="line">model = RNNTheano<span class="params">(vocabulary_size)</span></span><br><span class="line"><span class="built_in">%</span>timeit model.sgd_step<span class="params">(X_train[<span class="number">10</span>], y_train[<span class="number">10</span>], <span class="number">0.005</span>)</span></span><br></pre></td></tr></table></figure></p>
<p>相比之前的实现，在有GPU的Amazon EC2实例速度大约提高了15倍。为了帮读者节省时间，作者有一个训练好的模型在github仓库中的data/trained-model-theano.npz。用load_model_parameters_theano方法加载它。<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">from</span> utils <span class="import"><span class="keyword">import</span> load_model_parameters_theano, save_model_parameters_theano</span></span><br><span class="line"></span><br><span class="line"><span class="title">model</span> = <span class="type">RNNTheano</span>(vocabulary_size, hidden_dim=<span class="number">50</span>)</span><br><span class="line"><span class="preprocessor"># losses = train_with_sgd(model, X_train, y_train, nepoch=50)</span></span><br><span class="line"><span class="preprocessor"># save_model_parameters_theano('./data/trained-model-theano.npz', model)</span></span><br><span class="line"><span class="title">load_model_parameters_theano</span>('./<span class="typedef"><span class="keyword">data</span>/trained-model-theano.npz', model)</span></span><br></pre></td></tr></table></figure></p>
<h2 id="产生文本">产生文本</h2><p>现在我们可以利用模型来产生文本啦！让我们来实现一个辅助函数来产生句子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_sentence</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="comment"># We start the sentence with the start token</span></span><br><span class="line">    new_sentence = [word_to_index[sentence_start_token]]</span><br><span class="line">    <span class="comment"># Repeat until we get an end token</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> new_sentence[-<span class="number">1</span>] == word_to_index[sentence_end_token]:</span><br><span class="line">        next_word_probs = model.forward_propagation(new_sentence)</span><br><span class="line">        sampled_word = word_to_index[unknown_token]</span><br><span class="line">        <span class="comment"># We don't want to sample unknown words</span></span><br><span class="line">        <span class="keyword">while</span> sampled_word == word_to_index[unknown_token]:</span><br><span class="line">            samples = np.random.multinomial(<span class="number">1</span>, next_word_probs[-<span class="number">1</span>])</span><br><span class="line">            sampled_word = np.argmax(samples)</span><br><span class="line">        new_sentence.append(sampled_word)</span><br><span class="line">    sentence_str = [index_to_word[x] <span class="keyword">for</span> x <span class="keyword">in</span> new_sentence[<span class="number">1</span>:-<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">return</span> sentence_str</span><br><span class="line"></span><br><span class="line">num_sentences = <span class="number">10</span></span><br><span class="line">senten_min_length = <span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_sentences):</span><br><span class="line">    sent = []</span><br><span class="line">    <span class="comment"># We want long sentences, not sentences with one or two words</span></span><br><span class="line">    <span class="keyword">while</span> len(sent) &amp;lt; senten_min_length:</span><br><span class="line">        sent = generate_sentence(model)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">" "</span>.join(sent)</span><br></pre></td></tr></table></figure></p>
<p>选取了其中一些经过审核的句子，我加上了大小字母：</p>
<ul>
<li>Anyway, to the city scene you’re an idiot teenager.</li>
<li>What ? ! ! ! ! ignore!</li>
<li>Screw fitness, you’re saying: https</li>
<li>Thanks for the advice to keep my thoughts around girls.</li>
<li>Yep, please disappear with the terrible generation.<br>看起来我们的模型成功地学习了句法，它正确地放置了逗号以及结束标点符号。有时候还会模仿网络言论，例如多个感叹号或者微笑符。<br>然后，大部分产生的句子都没什么意义，而且有语法错误。（上面展示的已经是最好的了）。其中一个原因是我们没有训练足够长的时间或者数据量不够大，这可能是真的，但不太可能是主要原因。我们普通的RNN不能产生有意义的文字是因为它不能学习几个词以前的依赖关系。这也是RNN刚出现的时候不能得到流行的原因。这是很漂亮的理论，但是实际应用却不行，我们当时并不能立即理解这是为什么。<br>幸运的是，训练RNN的难点现在已经被更好地理解了。在下一部分教程中，我们将深入探索BPTT算法，并阐释什么叫vanishing gradient problem。这会激励我们去探索更复杂的RNN模型，比如LSTM，这是现在NLP许多应用领域最先进的方法。（而且还能产生好得多的reddit评论）在这个教程中学习的内容同样适用于LSTM和其他RNN模型，所以不要因为现在这个RNN模型的效果不好而感到沮丧。</li>
</ul>
<p>好了，教程到此位置，记得查看在github中查看代码。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/09/RNN教程,part2-实现一个RNN/" data-id="ciunpnnit00001bpl603b86im" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/10/24/RNN教程,part4,实现GRU:LSTM RNN/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2016/09/21/redux入门/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/MarkDown-语法/">MarkDown 语法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/React-todoList/">React todoList</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/css-三栏布局-自适应/">css 三栏布局 自适应</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/es6-笔记/">es6 笔记</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/javascript-效率/">javascript 效率</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mvc-javascript/">mvc javascript</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nodejs-express-mongodb-入门/">nodejs express mongodb 入门</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/学习笔记/">学习笔记</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/MarkDown-语法/" style="font-size: 10px;">MarkDown 语法</a> <a href="/tags/React-todoList/" style="font-size: 10px;">React todoList</a> <a href="/tags/css-三栏布局-自适应/" style="font-size: 10px;">css 三栏布局 自适应</a> <a href="/tags/es6-笔记/" style="font-size: 10px;">es6 笔记</a> <a href="/tags/javascript-效率/" style="font-size: 10px;">javascript 效率</a> <a href="/tags/mvc-javascript/" style="font-size: 10px;">mvc javascript</a> <a href="/tags/nodejs-express-mongodb-入门/" style="font-size: 10px;">nodejs express mongodb 入门</a> <a href="/tags/学习笔记/" style="font-size: 10px;">学习笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">18</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/10/24/RNN教程,part4,实现GRU:LSTM RNN/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/10/09/RNN教程,part2-实现一个RNN/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/09/21/redux入门/">(no title)</a>
          </li>
        
          <li>
            <a href="/2015/10/15/编译原理/">编译原理</a>
          </li>
        
          <li>
            <a href="/2015/08/19/垂直水平居中的方法/">垂直水平居中的方法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Jennings<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>