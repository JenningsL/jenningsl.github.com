<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>jenningL</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="原文地址：http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
递归神经网络（RNN）是在许多NLP任务中有着不错效果的流行模型。尽管流行，但是介绍RNN的原理和实现的资料却比较少，这就是这篇教程的目的。本文包括以下几个部分：

介绍RNN（本文）

基于Pytho">
<meta property="og:type" content="article">
<meta property="og:title" content="jenningL">
<meta property="og:url" content="http://yoursite.com/2016/10/24/RNN教程，part1-介绍RNN（译）/index.html">
<meta property="og:site_name" content="jenningL">
<meta property="og:description" content="原文地址：http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
递归神经网络（RNN）是在许多NLP任务中有着不错效果的流行模型。尽管流行，但是介绍RNN的原理和实现的资料却比较少，这就是这篇教程的目的。本文包括以下几个部分：

介绍RNN（本文）

基于Pytho">
<meta property="og:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg">
<meta property="og:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/Screen-Shot-2015-09-17-at-10.39.06-AM-1024x557.png">
<meta property="og:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/Screen-Shot-2015-09-17-at-11.44.24-AM-1024x349.png">
<meta property="og:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/bidirectional-rnn-300x196.png">
<meta property="og:image" content="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/Screen-Shot-2015-09-16-at-2.21.51-PM-272x300.png">
<meta property="og:updated_time" content="2016-10-24T08:53:19.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="jenningL">
<meta name="twitter:description" content="原文地址：http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
递归神经网络（RNN）是在许多NLP任务中有着不错效果的流行模型。尽管流行，但是介绍RNN的原理和实现的资料却比较少，这就是这篇教程的目的。本文包括以下几个部分：

介绍RNN（本文）

基于Pytho">
  
    <link rel="alternative" href="/atom.xml" title="jenningL" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">jenningL</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-RNN教程，part1-介绍RNN（译）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/24/RNN教程，part1-介绍RNN（译）/" class="article-date">
  <time datetime="2016-10-24T08:53:20.000Z" itemprop="datePublished">2016-10-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>原文地址：<a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a></p>
<p>递归神经网络（RNN）是在许多NLP任务中有着不错效果的流行模型。尽管流行，但是介绍RNN的原理和实现的资料却比较少，这就是这篇教程的目的。本文包括以下几个部分：</p>
<ul>
<li><p>介绍RNN（本文）</p>
</li>
<li><p>基于Python和Theano实现RNN</p>
</li>
<li><p>理解Backpropagation Through Time(BPTT)算法和vanishing gradient problem</p>
</li>
<li><p>实现一个GRU/LSTM RNN</p>
</li>
</ul>
<p>作为教程的一部分，我们会实现一个基于递归神经网络的语言模型。这个语言模型的应用有两方面：1.它能根据在现实中出现的概率对任意句子进行评分。这提供了语法判断的可能性。常用于机器翻译系统。2.它允许我们产生新的文本，根据莎士比亚的文集训练的模型能让我们产生类似莎士比亚风格的文字。这篇Andrej Karpathy写的有趣的文章展示了基于RNN的字符级别语言模型的能力。<br>我假设你已经对神经网络比较熟悉了，如果不是，这里有一篇教程。</p>
<h2 id="什么是RNNS？">什么是RNNS？</h2><p>RNNs背后的思想是利用序列信息。在传统的神经网络中，我们假设所有输入（输出）相互之间是独立的。但是对于很多任务来说，这是一个糟糕的主意。如果你想预测句子中的下一个词，那么你最好知道它的前一个词是什么。RNNs被称为递归神经网络，因为它对序列中的每个元素都执行同一个任务，根据之前的计算输出结果。或者我们可以理解为，RNN拥有关于目前为止所计算到的信息的记忆。理论上，RNN可以利用任意长度的序列的信息，但实际上我们会限制往回看的步数。这是一个典型的RNN的样子：</p>
<p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg" alt=""><br>A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature</p>
<p>以上的图表展示了RNN被展开为完整的网络的过程。展开就是书写整个序列的网络的意思。例如，如果我们关系的序列是5个单词的句子，那网络可以展开为5层神经网络，每层对应一个单词。RNN的相关计算公式如下：</p>
<ul>
<li><p>是在时间步t的输入。例如 可以是一个对应了句子中第二个单词的one-hot vector</p>
</li>
<li><p>是时间步t的隐藏状态（hidden state）这是网络的记忆。 是基于先前的隐藏状态和当前的输入计算的：。函数  通常是非线形的，如 tanh或ReLU。在计算第一个隐藏状态时，通常被初始化为全0。</p>
</li>
<li><p>是时间步t的输出。例如，我们要预测下一个出现的词时，输出就是所有备选单词的概率向量。</p>
</li>
</ul>
<p>有几个地方值得注意的：</p>
<ul>
<li><p>你可以认为隐藏状态是网络的记忆，捕捉了在此前的信息。 完全根据在t时的记忆。在实际应用中，这通常会更复杂，因为不能保存太多步以前的信息。</p>
</li>
<li><p>传统神经网络每一层使用的参数通常不一样，RNN却在全部步骤间共享着同样的参数( above) 。这反映了我们在每步执行同样的任务的事实，只是输入不同而已。这大大地减少了需要学习的参数。</p>
</li>
<li><p>上面的图表中，每一个时间步t都有一个输出，但这不是必须的。例如我们需要判断句子的情感倾向，则只关心最终输出，而不是每个词的输出。同样的，我们也许不是每一步都需要输入。RNN主要的特征在它的隐藏层，其中保存了序列的信息。</p>
</li>
</ul>
<h2 id="RNN能做什么？">RNN能做什么？</h2><p>最常用的RNN类型是LSTM，它在捕获长距离的依赖方面比普通的RNN表现要出色。但不要担心，LSTM在本质上和RNN是同一个东西，我们会在这个课程里开发，只是在计算隐藏状态时用了不同的方法。之后的文章会详细的介绍LSTM。下面是RNN在NLP领域的应用。</p>
<h3 id="语言建模和生产文本">语言建模和生产文本</h3><p>。。。</p>
<h3 id="机器翻译">机器翻译</h3><p>。。。</p>
<p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/Screen-Shot-2015-09-17-at-10.39.06-AM-1024x557.png" alt=""><br>RNN for Machine Translation. Image Source: <a href="http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf" target="_blank" rel="external">http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf</a></p>
<p>Research papers about Machine Translation:</p>
<ul>
<li>A Recursive Recurrent Neural Network for Statistical Machine Translation</li>
<li>Sequence to Sequence Learning with Neural Networks</li>
<li>Joint Language and Translation Modeling with Recurrent Neural Networks</li>
</ul>
<h3 id="语音识别">语音识别</h3><p>。。。</p>
<p>Research papers about Speech Recognition:</p>
<ul>
<li>Towards End-to-End Speech Recognition with Recurrent Neural Networks</li>
</ul>
<h3 id="产生图像描述">产生图像描述</h3><p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/Screen-Shot-2015-09-17-at-11.44.24-AM-1024x349.png" alt=""><br>Deep Visual-Semantic Alignments for Generating Image Descriptions. Source: <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/" target="_blank" rel="external">http://cs.stanford.edu/people/karpathy/deepimagesent/</a></p>
<h2 id="训练RNN">训练RNN</h2><p>训练RNN和训练传统神经网络类似。我们同样使用backpropagation算法，但有一些不同。因为在所有时间步间共享参数，每个输出的梯度步仅仅取决于当前的时间步，也取决于之前的时间步。例如，为了计算t=4时的梯度，我们需要反向传播3步，然后将累加这些梯度。这叫做 Backpropagation Through Time (BPTT)。普通的RNN在使用BPTT学习长距离依赖（如：距离很远的两个依赖的时间步）时有困难，因为vanishing/exploding gradient problem。有一些技巧去梳理这些问题，而且特定的RNN类型如LSTM就是专门为了解决这些问题产生的。</p>
<h2 id="RNN扩展">RNN扩展</h2><h3 id="Bidirectional_RNNs_双向">Bidirectional RNNs 双向</h3><p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/bidirectional-rnn-300x196.png" alt=""></p>
<p>Deep (Bidirectional) RNNs</p>
<h3 id="双向多层">双向多层</h3><p><img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/Screen-Shot-2015-09-16-at-2.21.51-PM-272x300.png" alt=""></p>
<h2 id="LSTM_networks">LSTM networks</h2><p>这是近年来很流行的。LSTM并没有在架构上和RNN有什么不同，但它使用了不同的函数来计算隐藏状态。LSTM中的记忆被称为cells，你可以把它们当作黑盒，接受之前的状态 和当前的输入 .作为黑盒的输入。在这些cells内部，决定什么记忆会被保留，什么记忆会被抹去。然后结合之前的状态，当前的记忆以及输入。结果显示，这些类型的单元在捕获长距离依赖方面很有效，LSTM一开始很容易让人困惑，感兴趣可以看this post has an excellent explanation</p>
<h2 id="结论">结论</h2><p>希望你已经对RNN有了基本的了解，下一篇文章，我们将使用Python和Theano实现第一个版本的语言模型</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/24/RNN教程，part1-介绍RNN（译）/" data-id="ciusb88yu0010bpplt59m43rj" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/10/27/RNN教程，part3，BPTT and Vanishing Gradients/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2016/10/24/RNN教程,part4,实现GRU:LSTM RNN/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/MarkDown-语法/">MarkDown 语法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/React-todoList/">React todoList</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/css-三栏布局-自适应/">css 三栏布局 自适应</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/es6-笔记/">es6 笔记</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/javascript-效率/">javascript 效率</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mvc-javascript/">mvc javascript</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nodejs-express-mongodb-入门/">nodejs express mongodb 入门</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/学习笔记/">学习笔记</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/MarkDown-语法/" style="font-size: 10px;">MarkDown 语法</a> <a href="/tags/React-todoList/" style="font-size: 10px;">React todoList</a> <a href="/tags/css-三栏布局-自适应/" style="font-size: 10px;">css 三栏布局 自适应</a> <a href="/tags/es6-笔记/" style="font-size: 10px;">es6 笔记</a> <a href="/tags/javascript-效率/" style="font-size: 10px;">javascript 效率</a> <a href="/tags/mvc-javascript/" style="font-size: 10px;">mvc javascript</a> <a href="/tags/nodejs-express-mongodb-入门/" style="font-size: 10px;">nodejs express mongodb 入门</a> <a href="/tags/学习笔记/" style="font-size: 10px;">学习笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">18</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/10/27/RNN教程，part3，BPTT and Vanishing Gradients/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/10/24/RNN教程，part1-介绍RNN（译）/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/10/24/RNN教程,part4,实现GRU:LSTM RNN/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/10/24/RNN教程,part2-实现一个RNN/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/09/21/redux入门/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Jennings<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>