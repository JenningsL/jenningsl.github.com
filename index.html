<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>jenningL</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Front-end developer">
<meta property="og:type" content="website">
<meta property="og:title" content="jenningL">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="jenningL">
<meta property="og:description" content="Front-end developer">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="jenningL">
<meta name="twitter:description" content="Front-end developer">
  
    <link rel="alternative" href="/atom.xml" title="jenningL" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">jenningL</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-RNN教程,part4,实现GRU:LSTM RNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/24/RNN教程,part4,实现GRU:LSTM RNN/" class="article-date">
  <time datetime="2016-10-24T06:54:06.000Z" itemprop="datePublished">2016-10-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="LSTM_网络">LSTM 网络</h2><p>在part3中，我们看到了“消失的梯度”问题是如何给标准的RNN学习长距离依赖时造成困难的。LSTM是专门设计来解决“消失的梯度”问题，其中使用了一个阈值的机制。为了更好地理解，让我们来看看LSTM是如何计算隐藏状态st的（我使用了o来代表点乘）：</p>
<p><img src="../imgs/rnn10.png" alt=""></p>
<p>这些等式看起很复杂，但实际上没那么难。首先，注意到LSTM层只是计算隐藏状态的另一种方法。在之前，我们计算隐藏层的公式是：</p>
<p><img src="../imgs/rnn11.png" alt=""></p>
<p>这个单元的输入是xt（当前时间步t的输入），和st-1（之前的隐藏状态）。输出是一个新的隐藏状态st。一个LSTM单元做的是完全相同的事情，只是用不同的方法。这是从宏观上来理解的关键。你可以从根本上将LSTM和GRU看作是一个黑盒子。给定当前的输入和之前的隐藏状态，它们通过某种方法计算下一个隐藏状态。</p>
<p><img src="../imgs/rnn12.png" alt=""></p>
<p>带着这个想法，让我们尝试去直观地感受LSTM是如何计算隐藏状态的。Chris Olah有<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">一篇很好的教程</a>，在这里就不在赘述了。我强烈推荐你去看一下这边细节的教程。但总结如下：</p>
<ul>
<li>i,f,o分别被称为input gates,forget gates和output gates。注意到它们的计算公式是一样的，只是参数矩阵不同。它们被称为gates是因为sigmoid函数将这些向量的值限制在0～1之间，然后通过将他们和其他向量点乘，你定义了你想让其他向量“通过多少（let through）”。input gate 定义了让多少 根据当前输入新计算的状态 通过。forget gate定义了让多少之前的状态通过。最后，output gate 定义了你想要暴露多少内部状态给外部网络（更高的层和下一个时间步）。所有的gates 都有同样的维度ds，也就是你的隐藏状态的维度。</li>
<li>g是一个“候选”隐藏状态，基于当前的输入和先前的隐藏状态计算得出。这和普通的RNN中的计算公式是一模一样的，我们只是把参数U和W改名成Ug和Wg而已。然后，我们不是直接将g作为新的隐藏状态，我们会使用上面提到的input gate来挑选其中的一部分。</li>
<li>ct是单元的内部记忆。这是（先前的记忆ct-1乘以forget gate） 以及 （新计算出的隐藏状态g） 的结合。因此，从直觉上来说，这代表了我们希望如何去组合先前的记忆和新的输入。我们可以选择完全遗忘旧的记忆（forget gate 全0）或者完全忽略新计算的隐藏状态（input<br>gate 全0），但大多数情况下我们会选择在这两个极端之间的某个方式。</li>
<li>给定了记忆ct，我们最终计算输出：隐藏状态st，通过将记忆和ouput gate相乘。不是所有的内部记忆都和网络中其他单元所使用的隐藏单元有关。</li>
</ul>
<p><img src="../imgs/rnn13.png" alt=""></p>
<p>直观来说，普通的RNN可以看作是LSTM的一种特殊情况。如果你把input gate设置为全1，forget gate 设置为全0（遗忘全部先前的记忆）并且output gate设置为全1（暴露全部记忆），你几乎得到了标准的RNN，除了一个额外的tanh会将输出限制一下。gating 机制是LSTM能够明确地对长距离依赖建模的原因。通过学习它的的gates的参数，网络学习了它的的记忆应该如何工作。<br>值得一提的是，现存很多LSTM算法的变体。一个常见的是创建一个peephole connections，允许gates步仅仅依赖于之前的隐藏状态st-1，同时也依赖于之前的内部状态ct-1，在gate计算公式里面增加了一个项。还有很多其他的变体。<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="external">LSTM: A Search Space Odyssey</a>总结评估了不同的LSTM算法。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/24/RNN教程,part4,实现GRU:LSTM RNN/" data-id="ciunpnnjv00021bplbwr2rpns" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-RNN教程,part2-实现一个RNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/09/RNN教程,part2-实现一个RNN/" class="article-date">
  <time datetime="2016-10-09T12:58:41.000Z" itemprop="datePublished">2016-10-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这是RNN教程的第二部分。</p>
<p><a href="https://github.com/dennybritz/rnn-tutorial-rnnlm" target="_blank" rel="external">github</a></p>
<p>这个部分我们会从头开始实现一个完全的递归神经网络，使用的是python，以及Theano来优化我们实现（这是一个执行GPU操作的库）。我会跳过一些样板代码，因为这对理解RNN没有什么帮助，但还是可以在github上找到它们。</p>
<h2 id="语言建模">语言建模</h2><p>我们的目标是使用RNN建立一个语言模型。假设我们有m个单词的句子，一个语言模型可以让我去预测观测到这个句子的概率（在一个给定的数据集中）：<br><img src="../img/rnn1.png" alt="概率公式"><br>句子的概率是(在给定前面出现的单词序列的条件下每个单词出现的概率)的乘积。所以，”He went to buy some chocolate” 这个句子的概率是”chocolate”出现在”He went to buy some”后面的概率乘以”some”出现在”He went to buy”后面的概率…以此类推。<br>这有什么用？为什么我们会想要计算一个句子的被观测到的概率？<br>首先，这样一个模型可以被用在打分机制中。例如，一个机器翻译系统通常会产生一些候选翻译结果，你可以使用这个模型去挑选一个最有可能的结果。从直觉上来说，概率最大的句子很有可能在语法上是正确的。在语音识别系统中也存在类似的打分。<br>但得到语言模型的同时也有一个很酷的副产物，因为我们可以预测给定前缀条件下一个单词出现的概率。我们可以产生新的文本。这是一个生产模型。给定一个现存的单词序列，我们根据预测概率选取最有可能的下一个单词，如此反复直到得到一个完整的句子。 关于这个，Andrej Karparthy写了一篇<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">很不错的文章</a>。他的模型是按字母粒度训练而不是按单词，可以产生从莎士比亚文集到Linux代码的任何东西。<br>注意到上面的等式，每个单词的出现概率取决于所有之前的单词。实际上，许多模型都因为计算复杂度和内存的限制很难表示这么长的依赖。它们通常会限制往回看的单词个数。RNN理论上可以捕获这么长的依赖，但是在实际上会更复杂些，我们以后会再讨论这个问题。</p>
<h2 id="训练数据和预处理">训练数据和预处理</h2><p>为了训练我们的语言模型，我们需要文本来学习。幸运的是，我们不需要标注，只需要原文本。我从<a href="https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_08" target="_blank" rel="external">Google’s BigQuery</a>下载了15000稍长的reddit评论。我们的模型产生的文本会看起来像reddit评论，希望如此。但正如大多数机器学习的项目，我们首先需要做一些预处理，把我们的数据转换为正确的格式。</p>
<h3 id="1-分词">1.分词</h3><p>我们有原始的文本，但我们想要基于单词粒度去做预测。这意味着我们必须将评论划分为句子，然后再划分为单词。我们可以简单地按照空格划分，但是这样不能正确处理标点符号。我们将使用<a href="http://www.nltk.org/" target="_blank" rel="external">NLTK</a>的word_tokenize和sent_tokenize 方法，它们将帮我们解决这个问题。</p>
<h3 id="2-移除罕见的单词">2.移除罕见的单词</h3><p>大多数单词在我们的文本中只会出现一到两次。移除这些罕见单词是个好主意。词汇量太大会增加训练的时间，而且我们没有很多这些单词上下文相关的例子，所以我们没办法学习到怎么正确地使用它们。这跟人类的学习方式是类似的。为了学习一个单词的使用方法，你必须在多个场景中见到它。<br>在我们的代码中，我限制我们的词汇为最常见的单词（8000）。我们将所有不在词汇里面的单词替换为UNKNOWN_TOKEN。例如，如果我们的词汇里没有包含”nonlinearities”这个单词，那么“nonlineraties are important in neural networks” 将变成 “UNKNOWN_TOKEN are important in Neural Networks”。UNKNOWN_TOKEN会成为我们词汇的一部分，我们会像其他单词一样预测它。当我们产生新的文本时，我们可以再次替换UNKNOWN_TOKEN。例如随机选取一个不在我们的词汇中的词，或者我们重新产生句子直到我们没有出现UNKNOWN_TOKEN为止。</p>
<h3 id="3-加入特殊的开始符和结束符">3.加入特殊的开始符和结束符</h3><p>我们同样希望学习到那些单词倾向于开始或者结束一个句子。为了做到这个，我们在句子前面加入了一个特殊的SENTENCE_START符号，在句子后面加入SENTENCE_END符号。这允许我们问：当出现SENTENCE_START时，下一个最可能出现的单词是什么？（句子中的第一个单词）</p>
<h3 id="4-建立训练数据矩阵">4.建立训练数据矩阵</h3><p>RNN的输入是向量，不是字符串。所以我们创建了单词和索引之间的映射:index_to_word，word_to_index。例如， 单词“friendly”可能在2001的下标，一个训练样本x可能像这样[0, 179, 341, 416]，其中0对应了SENTENCE_START。对应的标签y应该为[179, 341, 416, 1]。记住我们的目标是预测下一个单词，所以y只是x向量向左移一位，最后一个符号是SENTENCE_END。换句话说，单词179的预测应该是341（真实出现的下一个单词）。<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">vocabulary_size = <span class="number">8000</span></span><br><span class="line">unknown_token = <span class="string">"UNKNOWN_TOKEN"</span></span><br><span class="line">sentence_start_token = <span class="string">"SENTENCE_START"</span></span><br><span class="line">sentence_end_token = <span class="string">"SENTENCE_END"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the data and append SENTENCE_START and SENTENCE_END tokens</span></span><br><span class="line">print <span class="string">"Reading CSV file..."</span></span><br><span class="line"><span class="operator">with</span> <span class="built_in">open</span>(<span class="string">'data/reddit-comments-2015-08.csv'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    reader = csv.reader(f, skipinitialspace=True)</span><br><span class="line">    reader.next()</span><br><span class="line">    <span class="comment"># Split full comments into sentences</span></span><br><span class="line">    <span class="keyword">sentences</span> = itertools.chain(*[nltk.sent_tokenize(x[<span class="number">0</span>].decode(<span class="string">'utf-8'</span>).<span class="built_in">lower</span>()) <span class="keyword">for</span> x <span class="operator">in</span> reader])</span><br><span class="line">    <span class="comment"># Append SENTENCE_START and SENTENCE_END</span></span><br><span class="line">    <span class="keyword">sentences</span> = [<span class="string">"%s %s %s"</span> % (sentence_start_token, x, sentence_end_token) <span class="keyword">for</span> x <span class="operator">in</span> <span class="keyword">sentences</span>]</span><br><span class="line">print <span class="string">"Parsed %d sentences."</span> % (<span class="built_in">len</span>(<span class="keyword">sentences</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenize the sentences into words</span></span><br><span class="line">tokenized_sentences = [nltk.word_tokenize(sent) <span class="keyword">for</span> sent <span class="operator">in</span> <span class="keyword">sentences</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Count the word frequencies</span></span><br><span class="line">word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))</span><br><span class="line">print <span class="string">"Found %d unique words tokens."</span> % <span class="built_in">len</span>(word_freq.<span class="keyword">items</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the most common words and build index_to_word and word_to_index vectors</span></span><br><span class="line">vocab = word_freq.most_common(vocabulary_size-<span class="number">1</span>)</span><br><span class="line">index_to_word = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="operator">in</span> vocab]</span><br><span class="line">index_to_word.append(unknown_token)</span><br><span class="line">word_to_index = dict([(w,i) <span class="keyword">for</span> i,w <span class="operator">in</span> enumerate(index_to_word)])</span><br><span class="line"></span><br><span class="line">print <span class="string">"Using vocabulary size %d."</span> % vocabulary_size</span><br><span class="line">print <span class="string">"The least frequent word in our vocabulary is '%s' and appeared %d times."</span> % (vocab[-<span class="number">1</span>][<span class="number">0</span>], vocab[-<span class="number">1</span>][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace all words not in our vocabulary with the unknown token</span></span><br><span class="line"><span class="keyword">for</span> i, sent <span class="operator">in</span> enumerate(tokenized_sentences):</span><br><span class="line">    tokenized_sentences[i] = [w <span class="keyword">if</span> w <span class="operator">in</span> word_to_index <span class="keyword">else</span> unknown_token <span class="keyword">for</span> w <span class="operator">in</span> sent]</span><br><span class="line"></span><br><span class="line">print <span class="string">"\nExample sentence: '%s'"</span> % <span class="keyword">sentences</span>[<span class="number">0</span>]</span><br><span class="line">print <span class="string">"\nExample sentence after Pre-processing: '%s'"</span> % tokenized_sentences[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the training data</span></span><br><span class="line">X_train = np.asarray([[word_to_index[w] <span class="keyword">for</span> w <span class="operator">in</span> sent[:-<span class="number">1</span>]] <span class="keyword">for</span> sent <span class="operator">in</span> tokenized_sentences])</span><br><span class="line">y_train = np.asarray([[word_to_index[w] <span class="keyword">for</span> w <span class="operator">in</span> sent[<span class="number">1</span>:]] <span class="keyword">for</span> sent <span class="operator">in</span> tokenized_sentences])</span><br></pre></td></tr></table></figure></p>
<p>这是我们文本的真实训练样例：<br><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x:</span><br><span class="line">SENTENCE_START what <span class="keyword">are</span> n't you understanding about this ? !</span><br><span class="line"><span class="comment">[0, 51, 27, 16, 10, 856, 53, 25, 34, 69]</span></span><br><span class="line"></span><br><span class="line">y:</span><br><span class="line">what <span class="keyword">are</span> n't you understanding about this ? ! SENTENCE_END</span><br><span class="line"><span class="comment">[51, 27, 16, 10, 856, 53, 25, 34, 69, 1]</span></span><br></pre></td></tr></table></figure></p>
<h3 id="建造RNN">建造RNN</h3><p><img src="../imgs/rnn2.png" alt="overview"><br>接下来讲讲RNN的具体细节。输入x是单词的序列（就像上面打印的一样），每个xt是一个单词。但还有一件事：因为矩阵相乘的原因，我们不能简单地使用单词的索引（像36）作为输入，而是用vocabulary_size大小的one-hot vector来表示每个单词。例如下标是36的单词就是除了第36个元素是1外其他都是0。所以，每个xt是一个向量，x是一个矩阵，其中每一行代表一个单词。我们会在神经网络的代码中做这个转换而不是在预处理中。我们网络的输出o也是类似的格式，每个ot是一个vocabulary_size大小的向量，每个元素代表了对应下标的单词作为下一个单词出现的概率。<br>让我们来回顾以下RNN的计算公式：<br><img src="../imgs/rnn3.png" alt="equations"><br>我发现写下矩阵和向量的维度总是很有用的。让我们假设词汇库的大小C=8000，隐藏层的大小H=100。你可以认为隐藏层的大小是网络的记忆。更大隐藏层可以学习到更复杂的模式，但同时会增加计算量。我们有：<br><img src="../imgs/rnn4.png" alt="size"><br>这是很有用的信息。记住U，V和W是我们需要从数据中学习到的参数。因此我们总共需要学习2HC+H^2个参数。当C=8000，H=100时，这个数字是1610000。这个维度告诉我们模型的瓶颈。注意到xt是one-hot vector，用它和U相乘本质上是选择U中的一列，所以我们不需要执行完整的乘法。然后，最大的矩阵相乘是Vst，这也是我们希望尽量控制词汇库大小的原因。<br>有了以上的知识，我们开始实现这个模型。</p>
<h2 id="初始化">初始化</h2><p>我们从声明一个RNN类以及初始化参数开始。我将这个类称为RNNNumpy，因为之后会实现一个Theano版本的。初始化U，V和W需要一些技巧。我们不能将它们初始化为全0，因为那会导致在全部层中的对称计算。我们必须随机初始化它们。因为适当的初始化看起来对训练结果有很大的影响，所以很多这方面的研究。结果显示，最好的初始化方式是基于activation function（tanh in our case），而一个推荐的方法是在区间<img src="../imgs/rnn5.png" alt="interval">随机初始化这些权重，where n is the number of incoming connections from the previous layer（看不太懂，所以不翻了）。<br><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNNumpy</span>:</span></span><br><span class="line"></span><br><span class="line">    def __init__(<span class="variable">self</span>, word_dim, hidden_dim=<span class="number">100</span>, bptt_truncate=<span class="number">4</span>):<span class="preprocessor"></span><br><span class="line">        # Assign instance variables</span></span><br><span class="line">        <span class="variable">self</span>.word_dim = word_dim</span><br><span class="line">        <span class="variable">self</span>.hidden_dim = hidden_dim</span><br><span class="line">        <span class="variable">self</span>.bptt_truncate = bptt_truncate<span class="preprocessor"></span><br><span class="line">        # Randomly initialize the network parameters</span></span><br><span class="line">        <span class="variable">self</span>.U = np.random.uniform(-np.<span class="built_in">sqrt</span>(<span class="number">1</span>./word_dim), np.<span class="built_in">sqrt</span>(<span class="number">1</span>./word_dim), (hidden_dim, word_dim))</span><br><span class="line">        <span class="variable">self</span>.V = np.random.uniform(-np.<span class="built_in">sqrt</span>(<span class="number">1</span>./hidden_dim), np.<span class="built_in">sqrt</span>(<span class="number">1</span>./hidden_dim), (word_dim, hidden_dim))</span><br><span class="line">        <span class="variable">self</span>.W = np.random.uniform(-np.<span class="built_in">sqrt</span>(<span class="number">1</span>./hidden_dim), np.<span class="built_in">sqrt</span>(<span class="number">1</span>./hidden_dim), (hidden_dim, hidden_dim))</span><br></pre></td></tr></table></figure></p>
<p>在上面的代码中，word_dim是我们词汇库的大小， hidden_dim是隐藏层的大小，bptt_truncate将在之后解释。</p>
<h2 id="前向传播">前向传播</h2><p>现在我们来实现前向传播（预测单词的概率），根据计算公式的定义：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="comment"># The total number of time steps</span></span><br><span class="line">    T = len(x)</span><br><span class="line">    <span class="comment"># During forward propagation we save all hidden states in s because need them later.</span></span><br><span class="line">    <span class="comment"># We add one additional element for the initial hidden, which we set to 0</span></span><br><span class="line">    s = np.zeros((T + <span class="number">1</span>, self.hidden_dim))</span><br><span class="line">    s[-<span class="number">1</span>] = np.zeros(self.hidden_dim)</span><br><span class="line">    <span class="comment"># The outputs at each time step. Again, we save them for later.</span></span><br><span class="line">    o = np.zeros((T, self.word_dim))</span><br><span class="line">    <span class="comment"># For each time step...</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(T):</span><br><span class="line">        <span class="comment"># <span class="doctag">Note</span> that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.</span></span><br><span class="line">        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-<span class="number">1</span>]))</span><br><span class="line">        o[t] = softmax(self.V.dot(s[t]))</span><br><span class="line">    <span class="keyword">return</span> [o, s]</span><br><span class="line"></span><br><span class="line">RNNNumpy.forward_propagation = forward_propagation</span><br></pre></td></tr></table></figure></p>
<p>我们同时返回了输出o和隐藏状态s，它们之后会被用于计算梯度。但有时候，我们只需要最高概率的下一个单词，因此我们需要一个predict函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="comment"># Perform forward propagation and return index of the highest score</span></span><br><span class="line">    o, s = self.forward_propagation(x)</span><br><span class="line">    <span class="keyword">return</span> np.argmax(o, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">RNNNumpy.predict = predict</span><br></pre></td></tr></table></figure></p>
<p>我们来试一下这两个方法以及它们的输出：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">np<span class="class">.random</span><span class="class">.seed</span>(<span class="number">10</span>)</span><br><span class="line">model = <span class="function"><span class="title">RNNNumpy</span><span class="params">(vocabulary_size)</span></span></span><br><span class="line">o, s = model.<span class="function"><span class="title">forward_propagation</span><span class="params">(X_train[<span class="number">10</span>])</span></span></span><br><span class="line">print o<span class="class">.shape</span></span><br><span class="line">print o</span><br><span class="line"></span><br><span class="line">predictions = model.<span class="function"><span class="title">predict</span><span class="params">(X_train[<span class="number">10</span>])</span></span></span><br><span class="line">print predictions<span class="class">.shape</span></span><br><span class="line">print predictions</span><br></pre></td></tr></table></figure></p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(45, 8000)</span><br><span class="line">[[ <span class="number">0.00012408</span>  <span class="number">0.0001244</span>   <span class="number">0.00012603</span> ...,  <span class="number">0.00012515</span>  <span class="number">0.00012488</span></span><br><span class="line">   <span class="number">0.00012508</span>]</span><br><span class="line"> [ <span class="number">0.00012536</span>  <span class="number">0.00012582</span>  <span class="number">0.00012436</span> ...,  <span class="number">0.00012482</span>  <span class="number">0.00012456</span></span><br><span class="line">   <span class="number">0.00012451</span>]</span><br><span class="line"> [ <span class="number">0.00012387</span>  <span class="number">0.0001252</span>   <span class="number">0.00012474</span> ...,  <span class="number">0.00012559</span>  <span class="number">0.00012588</span></span><br><span class="line">   <span class="number">0.00012551</span>]</span><br><span class="line"> ...,</span><br><span class="line"> [ <span class="number">0.00012414</span>  <span class="number">0.00012455</span>  <span class="number">0.0001252</span>  ...,  <span class="number">0.00012487</span>  <span class="number">0.00012494</span></span><br><span class="line">   <span class="number">0.0001263</span> ]</span><br><span class="line"> [ <span class="number">0.0001252</span>   <span class="number">0.00012393</span>  <span class="number">0.00012509</span> ...,  <span class="number">0.00012407</span>  <span class="number">0.00012578</span></span><br><span class="line">   <span class="number">0.00012502</span>]</span><br><span class="line"> [ <span class="number">0.00012472</span>  <span class="number">0.0001253</span>   <span class="number">0.00012487</span> ...,  <span class="number">0.00012463</span>  <span class="number">0.00012536</span></span><br><span class="line">   <span class="number">0.00012665</span>]]</span><br><span class="line"></span><br><span class="line">   (45,)</span><br><span class="line">[<span class="number">1284 5221</span> <span class="number">7653 7430</span> <span class="number">1013 3562</span> <span class="number">7366 4860</span> <span class="number">2212 6601</span> <span class="number">7299 4556</span> <span class="number">2481 238 253</span>9</span><br><span class="line"> <span class="number">21 6548 26</span><span class="number">1 1780 200</span><span class="number">5 1810 53</span><span class="number">76 4146 47</span><span class="number">7 7051 48</span><span class="number">32 4991 89</span><span class="number">7 3485 21</span></span><br><span class="line"> <span class="number">7291 2007</span> <span class="number">6006 760</span> <span class="number">4864 2182</span> <span class="number">6569 2800</span> <span class="number">2752 6821</span> <span class="number">4437 7021</span> <span class="number">7875 6912</span> 3575]</span><br></pre></td></tr></table></figure>
<p>由于我们的随机初始化了U，V，W，所以现在预测是完全随机的。</p>
<h2 id="计算代价">计算代价</h2><p>我们使用代价函数L来衡量模型的表现，我们的目标是找到参数U，V，W，使得代价函数在我们的数据里最小。一个常见的选择是cross-entropy loss。如果我们有N个训练样本，和C个类别（词汇库的大小），那么预测o和标签y的代价函数是：<br><img src="../imgs/rnn6.png" alt="cost-function"><br>根据公式的定义，假设标签说第i个位置是正确的答案，也就是说y_n的第i个元素是1时，o_n的第i个元素如果为1，那么yn<em>logo_n=0，因为正确所以代价为0；反之，如果o_n的第i个元素比1小，那么yn</em>logo_n &lt; 0，所以代价不为0，而且o_n偏离1越远，代价越大。可以看到下面的实现中，因为y_n实际上是one-hot vector，所以我们只需要看正确的位置上对应的o_n即可，因为其他代价肯定是0。<br>以下是实现代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_total_loss</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    L = <span class="number">0</span></span><br><span class="line">    <span class="comment"># For each sentence...</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(len(y)):</span><br><span class="line">        o, s = self.forward_propagation(x[i])</span><br><span class="line">        <span class="comment"># We only care about our prediction of the "correct" words</span></span><br><span class="line">        correct_word_predictions = o[np.arange(len(y[i])), y[i]]</span><br><span class="line">        <span class="comment"># Add to the loss based on how off we were</span></span><br><span class="line">        L += -<span class="number">1</span> * np.sum(np.log(correct_word_predictions))</span><br><span class="line">    <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_loss</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    <span class="comment"># Divide the total loss by the number of training examples</span></span><br><span class="line">    N = np.sum((len(y_i) <span class="keyword">for</span> y_i <span class="keyword">in</span> y))</span><br><span class="line">    <span class="keyword">return</span> self.calculate_total_loss(x,y)/N</span><br><span class="line"></span><br><span class="line">RNNNumpy.calculate_total_loss = calculate_total_loss</span><br><span class="line">RNNNumpy.calculate_loss = calculate_loss</span><br></pre></td></tr></table></figure></p>
<p>让我们后退一步，思考一下随机预测的代价应该是多少。这会给我们一个参考基线，以及确保我们的实现是正确的。我们的词汇库大小是C，因此每个单词应该以1/C的概率被预测，进而得到代价是：<br><img src="../imgs/rnn7.png" alt="loss"><br>可以通过实验验证：<br><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Limit to 1000 examples to save time</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Expected Loss for random predictions: <span class="variable">%f</span>"</span> % np.<span class="keyword">log</span>(vocabulary_size)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Actual loss: <span class="variable">%f</span>"</span> % model.calculate_loss(X_train[:<span class="number">1000</span>], y_train[:<span class="number">1000</span>])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Expected Loss <span class="keyword">for</span> random <span class="string">predictions:</span> <span class="number">8.987197</span></span><br><span class="line">Actual <span class="string">loss:</span> <span class="number">8.987440</span></span><br></pre></td></tr></table></figure>
<h2 id="使用SGD和BPTT来训练RNN">使用SGD和BPTT来训练RNN</h2><p>记住我们的目标是寻找参数U，V，W使得在我们的数据中总代价最小。最常见的方法是随机梯度下降Stochastic Gradient Descent（SGD）。SGD背后的思想很简单。我们在每轮迭代中，遍历所有的训练样本并将参数推向一个使误差减少的方向。这个方向由误差（代价）的梯度<img src="../imgs/rnn8.png" alt="">来给出。SGD同样需要一个学习效率（learning rate），也就是每次迭代的步长。关于SGD的教程很多，这里不再赘述。<a href="http://cs231n.github.io/optimization-1/" target="_blank" rel="external">这里有个教程</a>。我将实现一个简单版本的SGD，即使是没有相关优化背景的人也能看懂。<br>但我们怎么计算上面提到的梯度？在传统神经网络中，我们使用反向传播算法来计算。在RNN中，我们使用一个稍微不同的算法叫Backpropagation Through Time(BPTT)。因为参数在所有时间步中被共享，每个输出的梯度不仅依赖于当前时间步，还依赖于之前的时间步。如果你懂微积分，这真的只是应用chain rule。下一部分教程全都是关于BPTT的，所以现在我们只需要将它当作一个黑盒子，接收训练样本(x, y)作为输入，返回梯度<img src="../imgs/rnn8.png" alt=""><br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bptt</span></span>(<span class="keyword">self</span>, x, y)<span class="symbol">:</span></span><br><span class="line">    <span class="constant">T </span>= len(y)</span><br><span class="line">    <span class="comment"># Perform forward propagation</span></span><br><span class="line">    o, s = <span class="keyword">self</span>.forward_propagation(x)</span><br><span class="line">    <span class="comment"># We accumulate the gradients in these variables</span></span><br><span class="line">    dLdU = np.zeros(<span class="keyword">self</span>.<span class="constant">U.</span>shape)</span><br><span class="line">    dLdV = np.zeros(<span class="keyword">self</span>.<span class="constant">V.</span>shape)</span><br><span class="line">    dLdW = np.zeros(<span class="keyword">self</span>.<span class="constant">W.</span>shape)</span><br><span class="line">    delta_o = o</span><br><span class="line">    delta_o[np.arange(len(y)), y] -= <span class="number">1</span>.</span><br><span class="line">    <span class="comment"># For each output backwards...</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(<span class="constant">T)</span>[<span class="symbol">:</span><span class="symbol">:-</span><span class="number">1</span>]<span class="symbol">:</span></span><br><span class="line">        dLdV += np.outer(delta_o[t], s[t].<span class="constant">T)</span></span><br><span class="line">        <span class="comment"># Initial delta calculation</span></span><br><span class="line">        delta_t = <span class="keyword">self</span>.<span class="constant">V.T.</span>dot(delta_o[t]) * (<span class="number">1</span> - (s[t] ** <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># Backpropagation through time (for at most self.bptt_truncate steps)</span></span><br><span class="line">        <span class="keyword">for</span> bptt_step <span class="keyword">in</span> np.arange(max(<span class="number">0</span>, t-<span class="keyword">self</span>.bptt_truncate), t+<span class="number">1</span>)[<span class="symbol">:</span><span class="symbol">:-</span><span class="number">1</span>]<span class="symbol">:</span></span><br><span class="line">            <span class="comment"># print "Backpropagation step t=%d bptt step=%d " % (t, bptt_step)</span></span><br><span class="line">            dLdW += np.outer(delta_t, s[bptt_step-<span class="number">1</span>])              </span><br><span class="line">            dLdU[<span class="symbol">:</span>,x[bptt_step]] += delta_t</span><br><span class="line">            <span class="comment"># Update delta for next step</span></span><br><span class="line">            delta_t = <span class="keyword">self</span>.<span class="constant">W.T.</span>dot(delta_t) * (<span class="number">1</span> - s[bptt_step-<span class="number">1</span>] ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> [dLdU, dLdV, dLdW]</span><br><span class="line"></span><br><span class="line"><span class="constant">RNNNumpy.</span>bptt = bptt</span><br></pre></td></tr></table></figure></p>
<h2 id="检查梯度">检查梯度</h2><p>在实现实现反向传播的同时实现梯度检查是一个好主意，它能够验证你的实现是否正确。梯度检查背后的思想是参数的微分等于该点的斜率，我们可以稍微改变参数然后除以这个改变，然后就可以得到微分的近似值：<br><img src="../imgs/rnn9.png" alt=""><br>然后我们使用反向传播计算的梯度和上面的方法计算的梯度进行比较，如果差别不大那就没问题。近似值需要计算每个参数的总代价，所以梯度检查是非常昂贵的操作（毕竟我们有超过1百万的参数需要学习）。所以让它在词汇库更小的模型中执行比较好：<br><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def gradient_check(self, <span class="keyword">x</span>, <span class="keyword">y</span>, h=<span class="number">0</span>.<span class="number">001</span>, error_threshold=<span class="number">0</span>.<span class="number">01</span>):</span><br><span class="line">    <span class="comment"># Calculate the gradients using backpropagation. We want to checker if these are correct.</span></span><br><span class="line">    bptt_gradients = self.bptt(<span class="keyword">x</span>, <span class="keyword">y</span>)</span><br><span class="line">    <span class="comment"># List of all parameters we want to check.</span></span><br><span class="line">    model_parameters = [<span class="string">'U'</span>, <span class="string">'V'</span>, <span class="string">'W'</span>]</span><br><span class="line">    <span class="comment"># Gradient check for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> pidx, pname in enumerate(model_parameters):</span><br><span class="line">        <span class="comment"># Get the actual parameter value from the mode, e.g. model.W</span></span><br><span class="line">        parameter = operator.attrgetter(pname)(self)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Performing gradient check for parameter <span class="variable">%s</span> with size <span class="variable">%d</span>."</span> % (pname, np.prod(parameter.shape))</span><br><span class="line">        <span class="comment"># Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...</span></span><br><span class="line">        it = np.nditer(parameter, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line">            ix = it.multi_index</span><br><span class="line">            <span class="comment"># Save the original value so we can reset it later</span></span><br><span class="line">            original_value = parameter[ix]</span><br><span class="line">            <span class="comment"># Estimate the gradient using (f(x+h) - f(x-h))/(2*h)</span></span><br><span class="line">            parameter[ix] = original_value + h</span><br><span class="line">            gradplus = self.calculate_total_loss([<span class="keyword">x</span>],[<span class="keyword">y</span>])</span><br><span class="line">            parameter[ix] = original_value - h</span><br><span class="line">            gradminus = self.calculate_total_loss([<span class="keyword">x</span>],[<span class="keyword">y</span>])</span><br><span class="line">            estimated_gradient = (gradplus - gradminus)/(<span class="number">2</span>*h)</span><br><span class="line">            <span class="comment"># Reset parameter to original value</span></span><br><span class="line">            parameter[ix] = original_value</span><br><span class="line">            <span class="comment"># The gradient for this parameter calculated using backpropagation</span></span><br><span class="line">            backprop_gradient = bptt_gradients[pidx][ix]</span><br><span class="line">            <span class="comment"># calculate The relative error: (|x - y|/(|x| + |y|))</span></span><br><span class="line">            relative_error = np.<span class="keyword">abs</span>(backprop_gradient - estimated_gradient)/(np.<span class="keyword">abs</span>(backprop_gradient) + np.<span class="keyword">abs</span>(estimated_gradient))</span><br><span class="line">            <span class="comment"># If the error is to large fail the gradient check</span></span><br><span class="line">            <span class="keyword">if</span> relative_error &amp;<span class="keyword">gt</span>; error_threshold:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Gradient Check ERROR: parameter=<span class="variable">%s</span> ix=<span class="variable">%s</span>"</span> % (pname, ix)</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"+h Loss: <span class="variable">%f</span>"</span> % gradplus</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"-h Loss: <span class="variable">%f</span>"</span> % gradminus</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Estimated_gradient: <span class="variable">%f</span>"</span> % estimated_gradient</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Backpropagation gradient: <span class="variable">%f</span>"</span> % backprop_gradient</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Relative Error: <span class="variable">%f</span>"</span> % relative_error</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            it.iternext()</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Gradient check for parameter <span class="variable">%s</span> passed."</span> % (pname)</span><br><span class="line"></span><br><span class="line">RNNNumpy.gradient_check = gradient_check</span><br><span class="line"></span><br><span class="line"><span class="comment"># To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.</span></span><br><span class="line">grad_check_vocab_size = <span class="number">100</span></span><br><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line">model = RNNNumpy(grad_check_vocab_size, <span class="number">10</span>, bptt_truncate=<span class="number">1000</span>)</span><br><span class="line">model.gradient_check([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure></p>
<h2 id="实现SGD">实现SGD</h2><p>既然我们已经可以计算参数的梯度了，那我们就可以实现SGD了。我将它分为两个步骤：</p>
<ol>
<li>一个函数sdg_step，用来计算一份数据的梯度然后执行更新</li>
<li>外层循环，用以遍历训练集，然后调整学习率<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Performs one step of SGD.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numpy_sdg_step</span><span class="params">(self, x, y, learning_rate)</span>:</span></span><br><span class="line">    <span class="comment"># Calculate the gradients</span></span><br><span class="line">    dLdU, dLdV, dLdW = self.bptt(x, y)</span><br><span class="line">    <span class="comment"># Change parameters according to gradients and learning rate</span></span><br><span class="line">    self.U -= learning_rate * dLdU</span><br><span class="line">    self.V -= learning_rate * dLdV</span><br><span class="line">    self.W -= learning_rate * dLdW</span><br><span class="line"></span><br><span class="line">RNNNumpy.sgd_step = numpy_sdg_step</span><br></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Outer SGD Loop</span></span><br><span class="line"><span class="comment"># - model: The RNN model instance</span></span><br><span class="line"><span class="comment"># - X_train: The training data set</span></span><br><span class="line"><span class="comment"># - y_train: The training data labels</span></span><br><span class="line"><span class="comment"># - learning_rate: Initial learning rate for SGD</span></span><br><span class="line"><span class="comment"># - nepoch: Number of times to iterate through the complete dataset</span></span><br><span class="line"><span class="comment"># - evaluate_loss_after: Evaluate the loss after this many epochs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_with_sgd</span><span class="params">(model, X_train, y_train, learning_rate=<span class="number">0.005</span>, nepoch=<span class="number">100</span>, evaluate_loss_after=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="comment"># We keep track of the losses so we can plot them later</span></span><br><span class="line">    losses = []</span><br><span class="line">    num_examples_seen = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(nepoch):</span><br><span class="line">        <span class="comment"># Optionally evaluate the loss</span></span><br><span class="line">        <span class="keyword">if</span> (epoch % evaluate_loss_after == <span class="number">0</span>):</span><br><span class="line">            loss = model.calculate_loss(X_train, y_train)</span><br><span class="line">            losses.append((num_examples_seen, loss))</span><br><span class="line">            time = datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"%s: Loss after num_examples_seen=%d epoch=%d: %f"</span> % (time, num_examples_seen, epoch, loss)</span><br><span class="line">            <span class="comment"># Adjust the learning rate if loss increases</span></span><br><span class="line">            <span class="keyword">if</span> (len(losses) &amp;gt; <span class="number">1</span> <span class="keyword">and</span> losses[-<span class="number">1</span>][<span class="number">1</span>] &amp;gt; losses[-<span class="number">2</span>][<span class="number">1</span>]):</span><br><span class="line">                learning_rate = learning_rate * <span class="number">0.5</span></span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Setting learning rate to %f"</span> % learning_rate</span><br><span class="line">            sys.stdout.flush()</span><br><span class="line">        <span class="comment"># For each training example...</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(y_train)):</span><br><span class="line">            <span class="comment"># One SGD step</span></span><br><span class="line">            model.sgd_step(X_train[i], y_train[i], learning_rate)</span><br><span class="line">            num_examples_seen += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>现在让我们来测试一下需要多长的训练时间：<br><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed<span class="params">(<span class="number">10</span>)</span></span><br><span class="line">model = RNNNumpy<span class="params">(vocabulary_size)</span></span><br><span class="line"><span class="built_in">%</span>timeit model.sgd_step<span class="params">(X_train[<span class="number">10</span>], y_train[<span class="number">10</span>], <span class="number">0.005</span>)</span></span><br></pre></td></tr></table></figure></p>
<p>在作者的笔记本电脑上，一步SGD要花350ms，我们有80000个训练数据样本，那一次遍历就要几个小时，多次次遍历要花几天甚至几周！而且相比真正的研究人员我们使用的数据已经算小的了。那怎么办呢？<br>幸运的是，有很多方法可以加速我们的代码。我们可以继续使用同样的模型，而让代码跑的更快，或者我们可以修改模型使得计算复杂度下降，或者两者都有。研究人员已经提出了很多优化模型计算复杂的方法，例如，使用<a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="external">hierarchical</a> softmax 或者<a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf" target="_blank" rel="external">adding projection layers to avoid the large matrix multiplications</a>。但我想保持我们模型简单，所以选择了第一种方法：使用GPU加速我们的代码。让我们尝试使用小数据集来检查代价是否真的有在减少。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># Train on a small subset of the data to see what happens</span></span><br><span class="line"><span class="variable">model =</span> RNNNumpy(vocabulary_size)</span><br><span class="line"><span class="variable">losses =</span> train_with_sgd(model, X_train[:<span class="number">100</span>], y_train[:<span class="number">100</span>], <span class="variable">nepoch=</span><span class="number">10</span>, <span class="variable">evaluate_loss_after=</span><span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015-09-30</span> 10:08:19: Loss after num_examples_seen=0 epoch=0: <span class="number">8.987425</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:08:35: Loss after num_examples_seen=100 epoch=1: <span class="number">8.976270</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:08:50: Loss after num_examples_seen=200 epoch=2: <span class="number">8.960212</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:09:06: Loss after num_examples_seen=300 epoch=3: <span class="number">8.930430</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:09:22: Loss after num_examples_seen=400 epoch=4: <span class="number">8.862264</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:09:38: Loss after num_examples_seen=500 epoch=5: <span class="number">6.913570</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:09:53: Loss after num_examples_seen=600 epoch=6: <span class="number">6.302493</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:10:07: Loss after num_examples_seen=700 epoch=7: <span class="number">6.014995</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:10:24: Loss after num_examples_seen=800 epoch=8: <span class="number">5.833877</span></span><br><span class="line"><span class="number">2015-09-30</span> 10:10:39: Loss after num_examples_seen=900 epoch=9: <span class="number">5.710718</span></span><br></pre></td></tr></table></figure>
<p>实验证明实现是符合预期的。</p>
<h2 id="使用Theano_和_GPU_训练我们的网络">使用Theano 和 GPU 训练我们的网络</h2><p>我之前写了一篇关于Theano的<a href="http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/" target="_blank" rel="external">教程</a>，鉴于所有的逻辑都是一样的，我不会在这里过一遍优化代码。我定义了一个RNNTheano类，用来取代原来的实现版本。代码同样能在<a href="https://github.com/dennybritz/rnn-tutorial-rnnlm" target="_blank" rel="external">github</a>上找到。<br><figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed<span class="params">(<span class="number">10</span>)</span></span><br><span class="line">model = RNNTheano<span class="params">(vocabulary_size)</span></span><br><span class="line"><span class="built_in">%</span>timeit model.sgd_step<span class="params">(X_train[<span class="number">10</span>], y_train[<span class="number">10</span>], <span class="number">0.005</span>)</span></span><br></pre></td></tr></table></figure></p>
<p>相比之前的实现，在有GPU的Amazon EC2实例速度大约提高了15倍。为了帮读者节省时间，作者有一个训练好的模型在github仓库中的data/trained-model-theano.npz。用load_model_parameters_theano方法加载它。<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">from</span> utils <span class="import"><span class="keyword">import</span> load_model_parameters_theano, save_model_parameters_theano</span></span><br><span class="line"></span><br><span class="line"><span class="title">model</span> = <span class="type">RNNTheano</span>(vocabulary_size, hidden_dim=<span class="number">50</span>)</span><br><span class="line"><span class="preprocessor"># losses = train_with_sgd(model, X_train, y_train, nepoch=50)</span></span><br><span class="line"><span class="preprocessor"># save_model_parameters_theano('./data/trained-model-theano.npz', model)</span></span><br><span class="line"><span class="title">load_model_parameters_theano</span>('./<span class="typedef"><span class="keyword">data</span>/trained-model-theano.npz', model)</span></span><br></pre></td></tr></table></figure></p>
<h2 id="产生文本">产生文本</h2><p>现在我们可以利用模型来产生文本啦！让我们来实现一个辅助函数来产生句子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_sentence</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="comment"># We start the sentence with the start token</span></span><br><span class="line">    new_sentence = [word_to_index[sentence_start_token]]</span><br><span class="line">    <span class="comment"># Repeat until we get an end token</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> new_sentence[-<span class="number">1</span>] == word_to_index[sentence_end_token]:</span><br><span class="line">        next_word_probs = model.forward_propagation(new_sentence)</span><br><span class="line">        sampled_word = word_to_index[unknown_token]</span><br><span class="line">        <span class="comment"># We don't want to sample unknown words</span></span><br><span class="line">        <span class="keyword">while</span> sampled_word == word_to_index[unknown_token]:</span><br><span class="line">            samples = np.random.multinomial(<span class="number">1</span>, next_word_probs[-<span class="number">1</span>])</span><br><span class="line">            sampled_word = np.argmax(samples)</span><br><span class="line">        new_sentence.append(sampled_word)</span><br><span class="line">    sentence_str = [index_to_word[x] <span class="keyword">for</span> x <span class="keyword">in</span> new_sentence[<span class="number">1</span>:-<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">return</span> sentence_str</span><br><span class="line"></span><br><span class="line">num_sentences = <span class="number">10</span></span><br><span class="line">senten_min_length = <span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_sentences):</span><br><span class="line">    sent = []</span><br><span class="line">    <span class="comment"># We want long sentences, not sentences with one or two words</span></span><br><span class="line">    <span class="keyword">while</span> len(sent) &amp;lt; senten_min_length:</span><br><span class="line">        sent = generate_sentence(model)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">" "</span>.join(sent)</span><br></pre></td></tr></table></figure></p>
<p>选取了其中一些经过审核的句子，我加上了大小字母：</p>
<ul>
<li>Anyway, to the city scene you’re an idiot teenager.</li>
<li>What ? ! ! ! ! ignore!</li>
<li>Screw fitness, you’re saying: https</li>
<li>Thanks for the advice to keep my thoughts around girls.</li>
<li>Yep, please disappear with the terrible generation.<br>看起来我们的模型成功地学习了句法，它正确地放置了逗号以及结束标点符号。有时候还会模仿网络言论，例如多个感叹号或者微笑符。<br>然后，大部分产生的句子都没什么意义，而且有语法错误。（上面展示的已经是最好的了）。其中一个原因是我们没有训练足够长的时间或者数据量不够大，这可能是真的，但不太可能是主要原因。我们普通的RNN不能产生有意义的文字是因为它不能学习几个词以前的依赖关系。这也是RNN刚出现的时候不能得到流行的原因。这是很漂亮的理论，但是实际应用却不行，我们当时并不能立即理解这是为什么。<br>幸运的是，训练RNN的难点现在已经被更好地理解了。在下一部分教程中，我们将深入探索BPTT算法，并阐释什么叫vanishing gradient problem。这会激励我们去探索更复杂的RNN模型，比如LSTM，这是现在NLP许多应用领域最先进的方法。（而且还能产生好得多的reddit评论）在这个教程中学习的内容同样适用于LSTM和其他RNN模型，所以不要因为现在这个RNN模型的效果不好而感到沮丧。</li>
</ul>
<p>好了，教程到此位置，记得查看在github中查看代码。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/10/09/RNN教程,part2-实现一个RNN/" data-id="ciunpnnit00001bpl603b86im" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-redux入门" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/09/21/redux入门/" class="article-date">
  <time datetime="2016-09-20T16:05:32.000Z" itemprop="datePublished">2016-09-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="reducer_function">reducer function</h3><p>根据前一个状态和 dispatch 的  action 计算出下一个状态树（state tree）的函数，函数签名如下：<br><code>(previousState, action) =&gt; newState</code><br>几个注意点：</p>
<ul>
<li>reducer 必须是纯函数</li>
<li>要处理未知类型的 action</li>
<li>要能初始化应用（action 为 undefined），可以用es6的默认参数</li>
</ul>
<h3 id="Store">Store</h3><p>Store 有以下职责:</p>
<ul>
<li>维持应用的 state；</li>
<li>提供 getState() 方法获取 state；</li>
<li>提供 dispatch(action) 方法更新 state；</li>
<li>通过 subscribe(listener) 注册监听器。 </li>
</ul>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> &#123; createStore &#125; <span class="keyword">from</span> <span class="string">'redux'</span>;</span><br><span class="line"><span class="keyword">const</span> store = createStore(aReducerFunction);</span><br><span class="line"></span><br><span class="line">store.dispatch(action);</span><br></pre></td></tr></table></figure>
<h3 id="展示组件和容器组件">展示组件和容器组件</h3><p>结合react和redux进行开发时， 推荐将组件分为展示组件和容器组件。展示组件仅仅根据传入的props进行渲染，这些展示组件不知道redux的存在，这样做的好处是这些组件就算在不使用redux时也可以复用。而容器组件则作为高级的组件使用redux和数据打交道。</p>
<h3 id="结合react使用">结合react使用</h3><p>首先，需要安装redux的react绑定库<br><code>npm install --save react-redux</code></p>
<p>几个关键点：</p>
<ol>
<li>我们需要获取从之前安装好的 react-redux 提供的 Provider，并且在渲染之前将根组件包装进 <provider>。这使得我们的 store 能为下面的组件所用。（在内部，这个是通过 React 的 “context” 特性实现。）</provider></li>
</ol>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">import</span> &#123; Provider &#125; <span class="keyword">from</span> <span class="string">'react-redux'</span></span><br><span class="line">...</span><br><span class="line">let store = createStore(todoApp);</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> rootElement = <span class="built_in">document</span>.getElementById(<span class="string">'root'</span>)</span><br><span class="line">render(</span><br><span class="line">  <span class="xml"><span class="tag">&lt;<span class="title">Provider</span> <span class="attribute">store</span>=<span class="value">&#123;store&#125;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">App</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">Provider</span>&gt;</span>,</span><br><span class="line">  rootElement</span><br><span class="line">)</span></span><br></pre></td></tr></table></figure>
<ol>
<li>接着，我们想要通过 react-redux 提供的 connect() 方法将包装好的组件连接到Redux。connect方法是为了给组件注入一些props,包括dispatch以及组件所需要的部分全局state。而筛选这部分的全局state的函数作为connect的参数传入。<figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">import &#123; connect &#125; <span class="keyword">from</span> 'react-redux';</span><br><span class="line">...</span><br><span class="line">class App extends Component &#123;</span><br><span class="line">  render() &#123;</span><br><span class="line">    // 通过调用 connect() 注入:</span><br><span class="line">    const &#123; dispatch, a, b &#125; = this.props</span><br><span class="line">    return (</span><br><span class="line">      <span class="variable">&lt;div&gt;</span></span><br><span class="line">		...</span><br><span class="line">      <span class="variable">&lt;/div&gt;</span></span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 筛选出组件需要的<span class="keyword">state</span></span><br><span class="line">function select(<span class="keyword">state</span>) &#123;</span><br><span class="line">  return &#123;</span><br><span class="line">    a: <span class="keyword">state</span>.a,</span><br><span class="line">    b: <span class="keyword">state</span>.b</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">export <span class="keyword">default</span> connect(select)(App);</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="用于生成action_creator_的函数">用于生成action creator 的函数</h3><p>当action creator数量比较多的时候，会出现很多重复的样板代码。我们可以通过一个函数来帮我们生成action creator：</p>
<pre><code><span class="function"><span class="keyword">function</span> <span class="title">makeActionCreator</span><span class="params">(type, <span class="rest_arg">...argNames</span>)</span> </span>{
  <span class="keyword">return</span> <span class="function"><span class="keyword">function</span><span class="params">(<span class="rest_arg">...args</span>)</span> </span>{
    let action = { type }
    argNames.forEach((arg, index) =&gt; {
      action[argNames[index]] = args[index]
    })
    <span class="keyword">return</span> action
  }
}

<span class="keyword">const</span> ADD_TODO = <span class="string">'ADD_TODO'</span>
<span class="keyword">const</span> EDIT_TODO = <span class="string">'EDIT_TODO'</span>
<span class="keyword">const</span> REMOVE_TODO = <span class="string">'REMOVE_TODO'</span>

export <span class="keyword">const</span> addTodo = makeActionCreator(ADD_TODO, <span class="string">'todo'</span>)
export <span class="keyword">const</span> editTodo = makeActionCreator(EDIT_TODO, <span class="string">'id'</span>, <span class="string">'todo'</span>)
export <span class="keyword">const</span> removeTodo = makeActionCreator(REMOVE_TODO, <span class="string">'id'</span>)
</code></pre><h3 id="Middleware_在_redux_中的应用">Middleware 在 redux 中的应用</h3><p><a href="http://cn.redux.js.org/docs/advanced/Middleware.html" target="_blank" rel="external">middleware</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/09/21/redux入门/" data-id="ciunpnnjt00011bplqlzjoq52" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-编译原理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/10/15/编译原理/" class="article-date">
  <time datetime="2015-10-15T02:53:24.000Z" itemprop="datePublished">2015-10-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/10/15/编译原理/">编译原理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="什么是编译器">什么是编译器</h2><p>编译器（compiler）就是一种程序，它可以读进一种语言编写的程序，然后翻译成另一种语言（目标语言）编写的程序。编译器也可以称为一种语言处理器。<br>而解析器（interpreter）则是另外一种语言处理器，它不通过翻译方式生成目标语言，而是直接利用用户提供的输入执行源程序中特定的操作。<br>编译器生成的目标程序通常比一个解析器执行的程序快，但是解析器的错误诊断效果比编译器好，因为它逐个语句执行源程序。</p>
<h2 id="编译器的结构">编译器的结构</h2><p>字符流－》［词法分析器］－符号流－》［语法分析］－语法树－》［语义分析］－语法树－》中间代码生成器－中间表示形式－》［机器无关代码优化器］－中间表示形式－》代码生成器－目标机器语言－》［机器相关代码优化器］－》目标机器语言</p>
<p>其中两个优化步骤是可选的</p>
<p>－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－</p>
<h2 id="词法分析器(Lexer)">词法分析器(Lexer)</h2><p>词法分析器的工作是将输入的字符串分割并识别为不同的token对，token对包括类别和值。如<identifier,"foo">。<br>一个词法分析器需要做两项工作：</identifier,"foo"></p>
<ul>
<li>分割字符串，识别出对应不同token的子字符串，亦即词汇(lexemes)</li>
<li>对每个词进行分类<br>有时候为了正确分类，look ahead是必要的。</li>
</ul>
<p>formal language 是从一个alphabet里面取出任意字符组成的一个字符串集。</p>
<p>在进行词法分析时</p>
<ul>
<li>在利用正则表达式来进行匹配时，通常是采取maximal munch 的策略，即读入尽可能长的输入进行匹配。match as long as possible.</li>
<li>建立一个有限级列表，当存在多个匹配时选择有限度最高的那个。将无法匹配的情况作为有限度最低的一种情况。</li>
</ul>
<h3 id="Finite_Automata">Finite Automata</h3><ul>
<li>Deterministic Finite Automata(DFA) 只能一次消耗一个（不能不消耗）字符然后进行转移，而且消耗的字符只有一种转移的选择。</li>
<li>Nondeterministic Finite Automata(NFA) 可以消耗一个或不消耗字符然后进行转移，转移规则可以是一个字符对应多个转移。</li>
</ul>
<p>DFA容易实现，执行很快；NFA执行较慢，但是规模远远小于DFA,因此通常采用NFA来实现正则表达式匹配.</p>
<p>正则表达式的实现：<br><img src="../img/regExp2NFA.png" alt="正则表达式的实现" title="正则表达式的实现"></p>
<p>从NFA到DFA的关键在于合并epsilon-closure,然后找出每个epsilon-closure之间的转换规则。</p>
<p>可以利用2维数组来实现DFA。</p>
<h2 id="语法分析器(Parser)">语法分析器(Parser)</h2><p>语法分析器接受词法分析器得到的token字符串作为输入，得到一个语法树。</p>
<p>文法：用于描述程序设计语言的构造的语法。一个上下文无关文法由终结符号、非终结符号、1个开始符号和一组产生式组成。</p>
<p>推导：从开始符号出发，不断将非终结符号替换为它的某个产生式的体的过程。</p>
<h3 id="自顶向下的语法分析">自顶向下的语法分析</h3><p>自顶向下的语法分析基本的方法是［递归下降语法分析］，这种方法可能需要回溯。<br><img src="../img/rdAlgorithm.png" alt="递归下降语法分析程序例子" title="递归下降语法分析程序例子"><br>如果一个文法中有一个非终结符号A使得某个串存在一个推导A =&gt; Aa,那么这个文法就是左递归的。如果文法存在左递归，则该算法会陷入无限循环调用（如下图），因此需要消除左递归。<br><img src="../img/leftRecursive.png" alt="左递归" title="左递归"></p>
<p>对于称为LL(1)的文法，我们可以构造出预测分析器，即不需要回溯的递归下降语法分析器。</p>
<h3 id="自底向上的语法分析">自底向上的语法分析</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/10/15/编译原理/" data-id="cifxwqvtm0008li2feumou9om" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/学习笔记/">学习笔记</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-垂直水平居中的方法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/08/19/垂直水平居中的方法/" class="article-date">
  <time datetime="2015-08-18T16:45:41.000Z" itemprop="datePublished">2015-08-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/19/垂直水平居中的方法/">垂直水平居中的方法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="如果需要居中的对象是文字或者图片，那么很简单，line-height_+_text-align_就可以了,如果是图片的话，还要对图片设置vertical-align:middle">如果需要居中的对象是文字或者图片，那么很简单，line-height + text-align 就可以了,如果是图片的话，还要对图片设置vertical-align:middle</h2><h2 id="inline-block_居中法">inline-block 居中法</h2><p>将div设置为display:inline-block,就可以用文字居中的方法来居中该div了<br><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;div id=<span class="string">"parent"</span>&gt;</span><br><span class="line">&lt;div id=<span class="string">"child"</span>&gt;&lt;/div&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">#parent &#123;<span class="variable">height</span>: <span class="number">200</span>px;</span><br><span class="line"><span class="built_in">line</span>-<span class="variable">height</span>:<span class="number">200</span>px;</span><br><span class="line"><span class="built_in">background</span>:<span class="built_in">blue</span>;</span><br><span class="line"><span class="built_in">text</span>-align:center;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#child&#123;</span><br><span class="line">display:inline-block;</span><br><span class="line"><span class="built_in">background</span>:<span class="built_in">red</span>;</span><br><span class="line"><span class="variable">width</span>: <span class="number">30</span>px;</span><br><span class="line"><span class="variable">height</span>:<span class="number">30</span>px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="绝对定位加负margin法">绝对定位加负margin法</h2><p>适用于需要居中的块级元素的宽高确定的情况。<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="id">#parent</span> <span class="rules">&#123;<span class="rule"><span class="attribute">position</span>:<span class="value"> relative</span></span>;&#125;</span></span><br><span class="line"><span class="id">#child</span> <span class="rules">&#123;</span><br><span class="line"><span class="rule"><span class="attribute">position</span>:<span class="value"> absolute</span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">top</span>:<span class="value"> <span class="number">50%</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">left</span>:<span class="value"> <span class="number">50%</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">height</span>:<span class="value"> <span class="number">30%</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">width</span>:<span class="value"> <span class="number">50%</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">margin</span>:<span class="value"> -<span class="number">15%</span> <span class="number">0</span> <span class="number">0</span> -<span class="number">25%</span></span></span>;</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="绝对定位、margin:auto、四角拉伸">绝对定位、margin:auto、四角拉伸</h2><p>同样适用于需要居中的块级元素的宽高确定的情况。<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="id">#parent</span> <span class="rules">&#123;</span><br><span class="line"><span class="rule"><span class="attribute">width</span>:<span class="value"> <span class="number">500px</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">height</span>:<span class="value"> <span class="number">500px</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">border</span>:<span class="value"> <span class="number">1px</span> solid red</span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">position</span>:<span class="value"> relative</span></span>;</span><br><span class="line">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="id">#child</span><span class="rules">&#123;</span><br><span class="line"><span class="rule"><span class="attribute">width</span>:<span class="value"> <span class="number">200px</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">height</span>:<span class="value"> <span class="number">200px</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">position</span>:<span class="value"> absolute</span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">left</span>:<span class="value"> <span class="number">0px</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">top</span>:<span class="value"> <span class="number">0px</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">right</span>:<span class="value"> <span class="number">0px</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">bottom</span>:<span class="value"> <span class="number">0px</span></span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">margin</span>:<span class="value"> auto</span></span>;</span><br><span class="line"><span class="rule"><span class="attribute">background-color</span>:<span class="value"> <span class="hexcolor">#cc9900</span></span></span>;</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="floater_div">floater div</h2><p>用一个浮动的div去居中目标div,</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="keyword">div</span> <span class="property">id</span>=<span class="string">"parent"</span>&gt;</span><br><span class="line">    &lt;<span class="keyword">div</span> <span class="property">id</span>=<span class="string">"floater"</span>&gt;&lt;/<span class="keyword">div</span>&gt;</span><br><span class="line">    &lt;<span class="keyword">div</span> <span class="property">id</span>=<span class="string">"child"</span>&gt;Content here&lt;/<span class="keyword">div</span>&gt;</span><br><span class="line">&lt;/<span class="keyword">div</span>&gt;</span><br><span class="line"><span class="comment">#parent &#123;height: 250px;</span></span><br><span class="line">background:blue;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#floater &#123;</span></span><br><span class="line">float: left;</span><br><span class="line">height: <span class="number">50</span>%;</span><br><span class="line">width: <span class="number">100</span>%;</span><br><span class="line">margin-bottom: -<span class="number">50</span>px; /*目标高度的一半*/</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#child &#123;</span></span><br><span class="line">margin: <span class="number">0</span> auto;</span><br><span class="line">width: <span class="number">100</span>px;</span><br><span class="line">clear: both;</span><br><span class="line">height: <span class="number">100</span>px;</span><br><span class="line">background:green;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>收集中…</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/08/19/垂直水平居中的方法/" data-id="cifxwqvu0000hli2f2lgi93sb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-三栏自适应布局" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/08/18/三栏自适应布局/" class="article-date">
  <time datetime="2015-08-18T01:59:27.000Z" itemprop="datePublished">2015-08-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/18/三栏自适应布局/">三栏自适应布局</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在平时的开发中，三栏布局的需求非常常见，包括垂直和水平。要求是：第一栏和最后一栏高度（或宽度）固定，中间一栏自适应。</p>
<h2 id="垂直三栏布局">垂直三栏布局</h2><p>中间一栏设置position: absolute,然后设置top,bottom,使得该栏高度被强制拉伸到自适应高度。具体原来还不是很了解，了解了再补上。<br>参考 <a href="http://www.cnblogs.com/yoyogis/p/4040513.html" target="_blank" rel="external">http://www.cnblogs.com/yoyogis/p/4040513.html</a></p>
<h2 id="水平三栏布局">水平三栏布局</h2><p>方法有三种：<br>参考 <a href="http://www.zhangxinxu.com/wordpress/2009/11/%E6%88%91%E7%86%9F%E7%9F%A5%E7%9A%84%E4%B8%89%E7%A7%8D%E4%B8%89%E6%A0%8F%E7%BD%91%E9%A1%B5%E5%AE%BD%E5%BA%A6%E8%87%AA%E9%80%82%E5%BA%94%E5%B8%83%E5%B1%80%E6%96%B9%E6%B3%95/" target="_blank" rel="external">http://www.zhangxinxu.com/wordpress/2009/11/%E6%88%91%E7%86%9F%E7%9F%A5%E7%9A%84%E4%B8%89%E7%A7%8D%E4%B8%89%E6%A0%8F%E7%BD%91%E9%A1%B5%E5%AE%BD%E5%BA%A6%E8%87%AA%E9%80%82%E5%BA%94%E5%B8%83%E5%B1%80%E6%96%B9%E6%B3%95/</a></p>
<p>方法一、左右两栏用绝对定位位于左右，中间一栏用margin撑开边距居中。<br>这个方法比较简单。不贴代码了<br>方法二、margin负值法<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="keyword">div</span> <span class="property">id</span>=<span class="string">"main"</span>&gt;</span><br><span class="line">&lt;<span class="keyword">div</span> <span class="property">id</span>=<span class="string">"body"</span>&gt;</span><br><span class="line">&lt;/<span class="keyword">div</span>&gt;</span><br><span class="line">&lt;/<span class="keyword">div</span>&gt;</span><br><span class="line">&lt;<span class="keyword">div</span> <span class="property">id</span>=<span class="string">"left"</span>&gt;&lt;/<span class="keyword">div</span>&gt;</span><br><span class="line">&lt;<span class="keyword">div</span> <span class="property">id</span>=<span class="string">"right"</span>&gt;&lt;/<span class="keyword">div</span>&gt;</span><br><span class="line"></span><br><span class="line">html,body&#123;</span><br><span class="line">height:<span class="number">100</span>%;</span><br><span class="line">width:<span class="number">100</span>%;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#main&#123;</span></span><br><span class="line">height:<span class="number">100</span>%;</span><br><span class="line">width:<span class="number">100</span>%;</span><br><span class="line">float:left;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#main #body&#123;</span></span><br><span class="line">margin:<span class="number">0</span> <span class="number">210</span>px;</span><br><span class="line">height:<span class="number">100</span>%;</span><br><span class="line">background:blue;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#left,#right&#123;</span></span><br><span class="line">width:<span class="number">200</span>px;</span><br><span class="line">float:left;</span><br><span class="line">height:<span class="number">100</span>%;</span><br><span class="line">background:green;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#left&#123;</span></span><br><span class="line">margin-left:-<span class="number">100</span>%;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#right&#123;</span></span><br><span class="line">margin-left:-<span class="number">200</span>px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>方法三、左右浮动，中间不浮动margin法<br>这种方法感觉是故意不触发中间的bfc,使得浮动元素覆盖中间元素，实际上感觉和方法一有点类似。<br><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;div id=<span class="string">"left"</span>&gt;&lt;/div&gt;</span><br><span class="line">&lt;div id=<span class="string">"right"</span>&gt;&lt;/div&gt;</span><br><span class="line">&lt;div id=<span class="string">"main"</span>&gt;&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">html,body&#123;</span><br><span class="line"><span class="label">height:</span><span class="number">100</span>%<span class="comment">;</span></span><br><span class="line"><span class="label">width:</span><span class="number">100</span>%<span class="comment">;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="preprocessor">#main&#123;</span></span><br><span class="line"><span class="label">height:</span><span class="number">100</span>%<span class="comment">;</span></span><br><span class="line"><span class="label">background:</span>blue<span class="comment">;</span></span><br><span class="line"><span class="label">margin:</span><span class="number">0</span> <span class="number">210</span>px<span class="comment">;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="preprocessor">#left&#123;</span></span><br><span class="line"><span class="label">width:</span><span class="number">200</span>px<span class="comment">;</span></span><br><span class="line"><span class="label">float:</span>left<span class="comment">;</span></span><br><span class="line"><span class="label">height:</span><span class="number">100</span>%<span class="comment">;</span></span><br><span class="line"><span class="label">background:</span>green<span class="comment">;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#right&#123;</span></span><br><span class="line"><span class="label">width:</span><span class="number">200</span>px<span class="comment">;</span></span><br><span class="line"><span class="label">float:</span>right<span class="comment">;</span></span><br><span class="line"><span class="label">height:</span><span class="number">100</span>%<span class="comment">;</span></span><br><span class="line"><span class="label">background:</span>yellow<span class="comment">;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/08/18/三栏自适应布局/" data-id="cifxwqvu4000jli2f0l6tvktz" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/css-三栏布局-自适应/">css 三栏布局 自适应</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-material-ui使用笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/08/05/material-ui使用笔记/" class="article-date">
  <time datetime="2015-08-05T06:53:38.000Z" itemprop="datePublished">2015-08-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/08/05/material-ui使用笔记/">material-ui使用笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>第一次见到material-ui的时候就被它的样式吸引了，它是基于react的组件，摸索了一下使用的方法，总结如下：</p>
<h3 id="bowserify加载">bowserify加载</h3><p>能够在浏览器用node的方法加载模块，很爽啊。。最终是打包生成一个js文件,html直接引用即可</p>
<h3 id="react的context属性">react的context属性</h3><p>material-ui的组件都需要使用context属性传入一个theme。react官方还没有正式发布这个属性的文档。<br><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="reserved">var</span>  React = <span class="built_in">require</span>(<span class="string">'react'</span>),</span><br><span class="line">  mui = <span class="built_in">require</span>(<span class="string">'material-ui'</span>),</span><br><span class="line">  RaisedButton = mui.RaisedButton,</span><br><span class="line">  AppBar = mui.AppBar,</span><br><span class="line">  ThemeManager = <span class="keyword">new</span> mui.Styles.ThemeManager();</span><br><span class="line"></span><br><span class="line"><span class="reserved">var</span>  SomeAwesomeComponent = React.createClass(&#123;<span class="attribute">displayName</span>: <span class="string">"SomeAwesomeComponent"</span>,</span><br><span class="line"></span><br><span class="line">  <span class="attribute">childContextTypes</span>: &#123;</span><br><span class="line">    <span class="attribute">muiTheme</span>: React.PropTypes.object</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  getChildContext() &#123;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">      <span class="attribute">muiTheme</span>: ThemeManager.getCurrentTheme()</span><br><span class="line">    &#125;;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  render() &#123;</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        React.createElement(AppBar, &#123;<span class="attribute">title</span>: <span class="string">"Title"</span>,<span class="attribute">iconClassNameRight</span>:<span class="string">"muidocs-icon-navigation-expand-more"</span>&#125;)</span><br><span class="line">    );</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<p>参考资料见：<a href="http://segmentfault.com/a/1190000002878442" target="_blank" rel="external">http://segmentfault.com/a/1190000002878442</a><br><a href="https://facebook.github.io/react/blog/2014/03/28/the-road-to-1.0.html#context" target="_blank" rel="external">https://facebook.github.io/react/blog/2014/03/28/the-road-to-1.0.html#context</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/08/05/material-ui使用笔记/" data-id="cifxwqvu8000nli2ffila46u8" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-es6笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/25/es6笔记/" class="article-date">
  <time datetime="2015-07-25T01:04:28.000Z" itemprop="datePublished">2015-07-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/25/es6笔记/">es6笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>＋ 尾部递归优化，只要在es6中使用尾部递归优化就不会发生调用栈溢出。</p>
<p>＋ WeakSet结构与Set类似，也是不重复的值的集合。但是，它与Set有两个区别。</p>
<p>首先，WeakSet的成员只能是对象，而不能是其他类型的值。</p>
<p>其次，WeakSet中的对象都是弱引用，即垃圾回收机制不考虑WeakSet对该对象的引用，也就是说，如果其他对象都不再引用该对象，那么垃圾回收机制会自动回收该对象所占用的内存，不考虑该对象还存在于WeakSet之中。这个特点意味着，无法引用WeakSet的成员，因此WeakSet是不可遍历的。<br>WeakSet的一个用处，是储存DOM节点，而不用担心这些节点从文档移除时，会引发内存泄漏。</p>
<p>＋ Map的键（是对象时）实际上是跟内存地址绑定的，只要内存地址不一样，就视为两个键。<br>如果Map的键是一个简单类型的值（数字、字符串、布尔值），则只要两个值严格相等，Map将其视为一个键，包括0和-0。另外，虽然NaN不严格相等于自身，但Map将其视为同一个键。</p>
<ul>
<li>Object.assign可以将源对象（source）的所有可枚举属性，复制到目标对象（target）。它至少需要两个对象作为参数，第一个参数是目标对象，后面的参数都是源对象。在克隆对象，合并对象，提供默认配置的情况都十分实用。例：Object.assign(target,source1,source2); </li>
<li>一个数据结构只要部署了Symbol.iterator方法，就被视为具有iterator接口，就可以用for…of循环遍历它的成员。for…of循环可以使用的范围包括数组、Set和Map结构、某些类似数组的对象（比如arguments对象、DOM NodeList对象）、后文的Generator对象，以及字符串。for..in遍历键名，for..of遍历键值<br>＋ 并不是所有类似数组的对象都具有iterator接口，一个简便的解决方法，就是使用Array.from方法将其转为数组。<br>＋ 对于普通的对象，for…of结构不能直接使用，会报错，必须部署了iterator接口后才能使用。但是，这样情况下，for…in循环依然可以用来遍历键名。<br>一种解决方法是，使用Object.keys方法将对象的键名生成一个数组，然后遍历这个数组。<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (var <span class="variable">key</span> of <span class="keyword">Object</span>.keys(someObject)) &#123;</span><br><span class="line">    console.<span class="built_in">log</span>(<span class="variable">key</span> + <span class="string">": "</span> + someObject[<span class="variable">key</span>]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>另一种方便的方法是将数组的Symbol.iterator属性，直接赋值给其他对象的Symbol.iterator属性<br><figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jQuery.prototype[<span class="type">Symbol</span>.<span class="keyword">iterator</span>] =</span><br><span class="line"><span class="type">Array</span>.prototype[<span class="type">Symbol</span>.<span class="keyword">iterator</span>];</span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>跟forEach相比，for..of可以与break、continue和return配合使用,而且能确保按顺序遍历，与for…in不同。for…in更适合于遍历对象的属性。</p>
</li>
<li><p>Class不存在变量提升（hoist），这一点与ES5完全不同。</p>
</li>
<li>es6使用extends关键字继承。<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ColorPoint</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">Point</span> &#123;</span></span><br><span class="line"></span><br><span class="line">constructor(x, y, color) &#123;</span><br><span class="line"><span class="keyword">super</span>(x, y); <span class="comment">// 调用父类的constructor(x, y)</span></span><br><span class="line"><span class="keyword">this</span>.color = color;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">toString() &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>.color + ' ' + <span class="keyword">super</span>.toString(); <span class="comment">// 调用父类的toString()</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>子类必须在constructor方法中调用super方法，否则新建实例时会报错。这是因为子类没有自己的this对象，而是继承父类的this对象，然后对其进行加工。如果不调用super方法，子类就得不到this对象。子类中的super指向父类实例。<br>ES5的继承，实质是先创造子类的实例对象this，然后再将父类的方法添加到this上面（Parent.apply(this)）。ES6的继承机制完全不同，实质是先创造父类的实例对象this（所以必须先调用super方法），然后再用子类的构造函数修改this。<br>＋ 在ES6之前，js内置的构造函数是不能被继承的，也就是说你不能定义一个Array的子类。这是因为原生构造函数无法外部获取，因此在子类中利用apply或者将原生构造函数分配给子类的原型对象都不行。而ES6允许继承原生构造函数定义子类，因为ES6是先新建父类的实例对象this，然后再用子类的构造函数修饰this，使得父类的所有行为都可以继承。这意味着我们这么定义一个Array的子类等等。<br>＋ 类相当于实例的原型，所有在类中定义的方法，都会被实例继承。如果在一个方法前，加上static关键字，就表示该方法不会被实例继承，而是直接通过类来调用，这就称为“静态方法”。</p>
<h2 id="Set,Map">Set,Map</h2><ul>
<li>Set 与Array类似，但是每个值不能重复（===），而且没有map,filter等方法，但是有forEach方法。需要使用map等方式时可以先用…将Set转成Array 操作完之后再转回Set(构造函数中参数直接传数组)</li>
<li>Map 与object类似，但是键可以为任意数据类型，即实现了 key - key 的映射关系。同样没有map,filter等方法</li>
<li>WeakMap结构与Map结构基本类似，唯一的区别是它只接受对象作为键名（null除外），不接受原始类型的值作为键名，而且键名所指向的对象，不计入垃圾回收机制。WeakMap的设计目的在于，键名是对象的弱引用（垃圾回收机制不将该引用考虑在内），所以其所对应的对象可能会被自动回收。WeakMap应用的典型场合就是DOM节点作为键名</li>
</ul>
<h2 id="Promise">Promise</h2><ul>
<li>如果Promise状态已经变成resolved，再抛出错误是无效的。</li>
<li>Promise对象的错误具有“冒泡”性质，会一直向后传递，直到被捕获为止。</li>
<li>跟传统的try/catch代码块不同的是，如果没有使用catch方法指定错误处理的回调函数，Promise对象抛出的错误不会传递到外层代码，即不会有任何反应。</li>
<li>如果Promise.resolve方法的参数，不是具有then方法的对象（又称thenable对象），则返回一个新的Promise对象，且它的状态为Resolved,而且Promise.resolve方法的参数，会同时传给回调函数。如：<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> p = <span class="built_in">Promise</span>.resolve(<span class="string">'Hello'</span>);</span><br><span class="line"></span><br><span class="line">p.then(<span class="function"><span class="keyword">function</span> (<span class="params">s</span>)</span>&#123;</span><br><span class="line"><span class="built_in">console</span>.log(s)</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// Hello</span></span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/25/es6笔记/" data-id="cifxwqvud000qli2fuks4a8o3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/es6-笔记/">es6 笔记</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-secret of javascript ninja读书笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/07/06/secret of javascript ninja读书笔记/" class="article-date">
  <time datetime="2015-07-06T11:25:00.000Z" itemprop="datePublished">2015-07-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/06/secret of javascript ninja读书笔记/">secret of javascript ninja读书笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="第四章－函数是一等对象">第四章－函数是一等对象</h2><p>一、由于函数可以当作普通的对象来使用，意味着可以在函数中存储属性。利用这个特性，可以实现两个技术：<br>1.存储不重复的函数集合（常用于任务队列中）<br>2.self-memoizing-function<br>存储已有的计算结果，每次调用函数时现查询cache中是否存在，节约计算成本。常见应用有：计算复杂的函数节省计算开销，选择dom元素的函数通过缓存改善性能。</p>
<p>二、函数有一个鲜为人知的属性－length,这个属性表示函数在声明时的参数个数。注意它跟arguments.length的区别。利用这两个属性，我们可以根据函数个数实现函数重载。<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">addMethod</span>(<span class="params">object,name,fn</span>)</span>&#123;</span><br><span class="line">	<span class="keyword">var</span> old=object[name];</span><br><span class="line">	object[name]=<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(fn.length==<span class="built_in">arguments</span>.length)</span><br><span class="line">			<span class="keyword">return</span> fn.apply(<span class="keyword">this</span>,<span class="built_in">arguments</span>);</span><br><span class="line">		<span class="keyword">else</span> <span class="keyword">if</span>(<span class="keyword">typeof</span> old==<span class="string">"function"</span>)&#123;</span><br><span class="line">			<span class="keyword">return</span> old.apply(<span class="keyword">this</span>,<span class="built_in">arguments</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*用法*/</span></span><br><span class="line"><span class="keyword">var</span> ninja=&#123;&#125;;</span><br><span class="line">addMethod(ninja,<span class="string">'whatever'</span>,<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">	<span class="comment">//do something</span></span><br><span class="line">&#125;);</span><br><span class="line">addMethod(ninja,<span class="string">'whatever'</span>,<span class="function"><span class="keyword">function</span>(<span class="params">a</span>)</span>&#123;</span><br><span class="line">	<span class="comment">//do something</span></span><br><span class="line">&#125;);</span><br><span class="line">addMethod(ninja,<span class="string">'whatever'</span>,<span class="function"><span class="keyword">function</span>(<span class="params">a,b</span>)</span>&#123;</span><br><span class="line">	<span class="comment">//do something</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p>
<p>这个例子实现了一个函数，可以给对象添加多个同名方法，在调用时该方法将自动根据参数的个数选择执行的版本。原理是每次添加一个函数时，使用一个闭包将之前版本的函数保存，然后将对象的该方法赋值为一个函数，这个函数根据当前所添加的函数在定义时的参数数量以及调用时实际传入的参数数量进行比较，决定是调用最外层的函数还是向之前的闭包寻找适合的函数。</p>
<h2 id="如何判断一个对象是不是函数实例">如何判断一个对象是不是函数实例</h2><p>最简单的方法 typeof<br>但是这个问题在一些浏览器下会有一些问题：<br>＋ Firefox html/<object> 的typeof求值结果是function</object></p>
<ul>
<li>IE 当尝试获取另一个不复存在的window（iframe）的function对象时，会返回unknown</li>
<li>safari 的dom nodelist 会被认为是function<br>最好的解决方法是 Object.prototype.toString().call(fn) 如果是函数，会返回”[object Function]”</li>
</ul>
<p>三、实现递归函数时，如何指向当前函数？<br>直接引用名字是不可靠的，利用arguments.callee据说以后会被抛弃。这里介绍了一种方法叫 inline function<br><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ninja=<span class="function"><span class="keyword">function</span> <span class="title">myNinja</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(终止条件)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	<span class="keyword">else</span> </span><br><span class="line">		<span class="keyword">return</span> myNinja();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里的myNinja只能被函数内部的作用域引用到</p>
<h2 id="第五章－闭包">第五章－闭包</h2><ul>
<li>前面几部分关于闭包的内容都是很常见的，比如 创建私有变量，修改函数执行环境。。。</li>
<li>闭包可以用来实现partial function(部分调用函数)和currying(函数柯里化)，两者的区别在于，partial可以任意安排预先提供的参数的位置，比如<br><code>fn.partial(a,undefined,b)</code><br>undefined的参数将在正式调用时提供.<br>而curry则只能预先提供前一个或几个参数</li>
<li>立即执行函数可以用来防止全局变量被污染.<h2 id="第九章_runtime_code_evaluation">第九章 runtime code evaluation</h2>动态解析运行js代码有几种方法：</li>
<li>eval()</li>
<li>Function构造函数,接受的最后一个参数作为函数主体部分的代码,前面的参数作为函数的参数。e.g var a = new Function(‘a’,’b’,’return a+b’);</li>
<li>插入script标签，令script.text = 代码;</li>
<li>定时器</li>
</ul>
<p>由于js能够动态地解析运行代码，我们可以利用这个特性去实现一些高级用法，比如将其他语言转译成js再执行、动态重写或注入代码。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/07/06/secret of javascript ninja读书笔记/" data-id="cifxwqvu6000mli2fuu5uoyr2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-React-Boostrap-jQuery快速搭建toDoList" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/05/React-Boostrap-jQuery快速搭建toDoList/" class="article-date">
  <time datetime="2015-05-05T04:35:52.000Z" itemprop="datePublished">2015-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/05/React-Boostrap-jQuery快速搭建toDoList/">React+Boostrap+jQuery快速搭建toDoList</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近看到Facebook 发布的React Native感觉好强大，同时也关注到了React这个框架，进行了一些了解之后发现这个东西真的挺好用。上手不算难，上手之后能够大大地提高我们的复杂webapp的开发效率。<br>经过几个小时的学习，已经能自己写出一个todolist应用了，算是一个学习的总结吧。<br>源码我已经发布到了github上，<a href="https://github.com/JenningsL/todolistApp" target="_blank" rel="external">todolist源码地址</a> 使用boostrap和jQuery只是为了美观和方便，并不是React依赖的。</p>
<p>下面是一些学习的总结：</p>
<ul>
<li>React一个优势就是实现组件的可重用，所以组件的划分非常重要，尽量使不同的组件之间松耦合。</li>
<li>由于React的数据是从组件架构的上层向下层单向流动的，所以控制应用状态的state应该放在所有子组件的共同父组件中，然后在父组件中的Render函数中通过props传递下去</li>
<li>有时候希望将数据从子组件向上流动，比如说这个todolist里面删除一个项目时希望将对应的id传递给最上层组件，然后进行state的更新。可以通过：父组件将回调函数作为props传给子组件，子组件在事件处理函数中调用该回调函数，然后将数据作为参数传递给父组件，父组件就可以利用这个参数进行更新了。</li>
<li>事件绑定是通过<code>onClick={this.handleDelete}</code>这样的语句写在xml标签里面的</li>
<li>异步编程时，比如写动画时，要确保state更新的时机正确。参考代码中的删除淡出效果，是在动画完成之后再setState</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/05/05/React-Boostrap-jQuery快速搭建toDoList/" data-id="cifxwqvuj000uli2ff7dmxp52" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/React-todoList/">React todoList</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/MarkDown-语法/">MarkDown 语法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/React-todoList/">React todoList</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/css-三栏布局-自适应/">css 三栏布局 自适应</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/es6-笔记/">es6 笔记</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/javascript-效率/">javascript 效率</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mvc-javascript/">mvc javascript</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nodejs-express-mongodb-入门/">nodejs express mongodb 入门</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/学习笔记/">学习笔记</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/MarkDown-语法/" style="font-size: 10px;">MarkDown 语法</a> <a href="/tags/React-todoList/" style="font-size: 10px;">React todoList</a> <a href="/tags/css-三栏布局-自适应/" style="font-size: 10px;">css 三栏布局 自适应</a> <a href="/tags/es6-笔记/" style="font-size: 10px;">es6 笔记</a> <a href="/tags/javascript-效率/" style="font-size: 10px;">javascript 效率</a> <a href="/tags/mvc-javascript/" style="font-size: 10px;">mvc javascript</a> <a href="/tags/nodejs-express-mongodb-入门/" style="font-size: 10px;">nodejs express mongodb 入门</a> <a href="/tags/学习笔记/" style="font-size: 10px;">学习笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">18</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/10/24/RNN教程,part4,实现GRU:LSTM RNN/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/10/09/RNN教程,part2-实现一个RNN/">(no title)</a>
          </li>
        
          <li>
            <a href="/2016/09/21/redux入门/">(no title)</a>
          </li>
        
          <li>
            <a href="/2015/10/15/编译原理/">编译原理</a>
          </li>
        
          <li>
            <a href="/2015/08/19/垂直水平居中的方法/">垂直水平居中的方法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Jennings<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>